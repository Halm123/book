\chapter{Differentiability}

The main idea is to just use the vector space structure of $\mathbb{R}^n$ to define a notion of differential. And then recover differentiability as maps that preserve that notion. Given that I am using only the vector space structure, letâ€™s just consider vector spaces. I would not be surprised if something along these lines has already been done.

\section{Differentials}

\begin{defn}
Let $V$ be a real vector space and $v \in V$ a vector. A differential $dV|_v$ is a sequence of vector $\{v_i\}_{i=1}^{\infty}$ that can be understood, in the limit, as a recursive subdivision that tends to $v$. That is, the limit $\lim\limits_{i \to \infty} \frac{v_i}{(1/2)^i} = \hat{v}$ exists. Two differentials are equivalent if the respective limits converge to the same vector.
\end{defn}

\begin{prop}
	The space of all differentials at $v$ is a vector space of the same dimensionality as $V$.
\end{prop}

\begin{proof}
	First we show that the differentials form a vector space. A linear combination $a dV^1 + b dV^2$ of two differentials is a differential since we have:
	$$\lim\limits_{i \to \infty} \frac{a v_i^1 + b v_i^2}{(1/2)^i} = a\lim\limits_{i \to \infty} \frac{v_i^1}{(1/2)^i} + b\lim\limits_{i \to \infty} \frac{v_i^2}{(1/2)^i} = a \hat{v}^1 + b \hat{v}^2$$.
	All properties of linear combinations of differentials become properties of the limit vectors, which means the differentials will satisfy the basic definitions of a vector space.
	
	Now we show that the dimensionality is the same. Take a basis $e_j$ for the vector space. For each basis vector we can define the sequence $d(e_j) = \{(1/2)^i e_j\}_{i=1}^{\infty}$. These sequences are differentials since
	$$\lim\limits_{i \to \infty} \frac{(1/2)^i e_j}{(1/2)^i} = \lim\limits_{i \to \infty} e_j = e_j.$$
	The span of these differentials cover all possible convergence limits and therefore all convergence classes. Any differential is therefore equivalent to a linear combination of the basis.
\end{proof}

\begin{remark}
	If we pick $\mathbb{R}$ as a our vector space, we can use $1$ as the basis and define the differential $dx=\{(1/2)^i 1\}_{i=1}^{\infty}=\{(1/2)^i\}_{i=1}^{\infty}$. In $\mathbb{R}^n$, $d(e_j)$ can then be written as $dx^j e_j$. That is, the limit is expressed only as a limit of the component.
\end{remark}

\begin{defn}
	Let $V$ and $W$ be two vector spaces. We say that a map $f: V \to W$ is differentiable if it maps differentials of $V$ to differentials of $W$. That is, if $dV|_v=\{v_i\}_{i=1}^{\infty}$ is a differential of $V$ at $v$, then $dW|_{f(v)} = \{f(v_i +v) - f(v)\}_{i=1}^{\infty}$ is a differential of $W$ at $f(v)$.
\end{defn}

\begin{prop}
	Let $f:\mathbb{R} \to \mathbb{R}$ and let it be differentiable according to the above definition. Then it is differentiable in the standard definition (i.e. existence of the derivative).
\end{prop}
\begin{proof}
	We start with a non-zero differential $dx|_x = \{x^i\}_{i=1}^{\infty}$ at $x$, which means $\lim\limits_{i \to \infty} \frac{x_i}{(1/2)^i} = \hat{x}$ exists and is not zero. The map induces the differential $dy|_{f(x)} = \{f(x_i +x) - f(x)\}_{i=1}^{\infty}$, which means $\lim\limits_{i \to \infty} \frac{f(x_i +x) - f(x)}{(1/2)^i} = \hat{y}$ converges. We can write:
	$$ \lim\limits_{i \to \infty} \frac{f(x_i +x) - f(x)}{(1/2)^i} = \lim\limits_{i \to \infty} \frac{f(x_i +x) - f(x)}{x_i}\frac{x_i}{(1/2)^i}.$$
	We have a convergent sequence that is a product of two sequences, one that is convergent. This means that $\lim\limits_{i \to \infty} \frac{f(x_i +x) - f(x)}{x_i}$ converges as well.
\end{proof}

\section{Linear functionals}

Review notation. Let $M$ be a differentiable manifold of dimension $n$.

\begin{defn}
	A $k$-surface is a $k$-dimensional smooth submanifold of $M$. We denote by $S^k$ the set of all  $k$-surfaces of dimension $k$ and by $S = \bigcup_{k=0}^n S^k$ the set of all smooth surfaces of all dimensions.
\end{defn}

\begin{defn}
	Given a $k$-surface $\sigma^k \in S^k$, the \textbf{boundary} of $\sigma^k$, denoted by $\partial\sigma^k \in S^{k-1}$ is the limit of varied coordinates. The \textbf{boundary operator} $\partial : S \to S$ is a map from a $k$-surface to its boundary. A surface is \textbf{closed} if has no boundary.
\end{defn}

\begin{coro}
	Boundary of smooth surfaces are smooth surfaces. Boundaries do not have boundaries. That is, $\partial\partial \sigma^k = \emptyset$ for all $\sigma^k \in S^k$.
\end{coro}


\begin{defn}
	A \textbf{$k$-functional} is a linear function of $k$-surfaces. That is, it is a function $f_k : S^k \to \mathbb{R}$ with the following properties:
	\begin{description}
		\item[Linear] $f_k(\sigma^k_1 \cup \sigma^k_2) = f_k(\sigma^k_1) + f_k(\sigma^k_2)$ for every $\sigma^k_1, \sigma^k_2 \in S^k$ such that $\sigma^k_1 \cap \sigma^k_2 = \emptyset$
		\item[No contribution from boundary] $f_k(\sigma^k) = f_k(\sigma^k \setminus \partial \sigma^k)$ for every $\sigma^k \in S^k$
		\item[Commutes with the limit] $\lim\limits_{i \to \infty} f_k(\sigma_i^k) = f_k(\lim\limits_{i \to \infty}\sigma_i^k)$
	\end{description}
	We denote by $F_k$ the set of all $k$-functionals of dimension $k$ and by $F = \bigcup_{k=0}^nF_k$ is the set of all $k$-functionals.
\end{defn}

\begin{coro}
	Any $k$-functional applied to the empty set returns zero. That is, for any $f_k \in F$, $f_k(\emptyset) = 0$.
\end{coro}

\begin{defn}
	The \textbf{zero $k$-functional}, noted $0_k \in F_k$, is the $k$-functional that always returns zero. That is, $0_k(\sigma^k) = 0$ for all $\sigma^k \in S^k$.
\end{defn}


\begin{defn}
	Given a $k$-functional $f_k \in F_k$, the \textbf{boundary functional} $\partial f_k \in F_{k+1}$ is a $(k+1)$-functional that applies $f_k$ on the boundary. That is, $\partial f_k(\sigma^{k+1}) = f_k(\partial \sigma^{k+1})$. 
\end{defn}

\begin{coro}
	The boundary functional of the boundary functional is the zero functional. That is, for any $k$-functional $f_k \in F$, $\partial \partial f_k = 0_{k+2}$.
\end{coro}

\begin{proof}
	$\partial \partial f_k (\sigma ^{f+2}) = \partial f_k (\partial \sigma ^{f+2}) = f_k (\partial \partial \sigma ^{f+2}) = f_k(\emptyset) = 0$
\end{proof}

\begin{defn}
	A $k$-surface $\sigma^k \in S^k$ is \textbf{contractible} if it can be continuously shrunk to a point. That is, the inclusion map $\iota : \sigma^k \to X$ is null-homotopic.
\end{defn}

\begin{defn}
	An \textbf{exact functional} is a $k$-functional that returns zero on all closed $k$-surfaces. That is, $f_k(\sigma^k) = 0$ for all $\sigma^k \in S^k$ such that $\partial\sigma^k = \emptyset$. A \textbf{closed functional} returns zero on all contractible closed surfaces.
\end{defn}

\begin{remark}
	Names are chosen to agree with exact/closed forms... Should we find better names?
\end{remark}

\begin{prop}
	Let $f \in F_k$ be an exact $k$-functional. Then there exists some $(k-1)$-functional $g \in F_{k-1}$ such that $f = \partial g$. We say $g$ is the \textbf{potential} of $f$.
\end{prop}

\begin{remark}
	The aim here is to prove the theorem on finite surfaces, without using standard differentiable calculus.
	
	As a model, we should use the standard proof used in physics for irrotational fields. Suppose we have an exact 1-functional, that is it gives zero for all closed lines. Then, one can show that two lines that share the same boundary must have the same value. Then pick a point and assign zero to that point. To any other point, assign the value given by the functional over a line that starts at the zero point and ends that the new point. The potential is given by those assignments.
	
	The way to generalize is to realize that a $k$-surface is half a boundary of a $k+1$-surface. For example, a point is half a boundary of a line, which constitutes of 2 points. The boundary of a surface is a closed line, which can be understood as two lines that share the same boundary, but opposite orientation. Like the line integral can be understood as ``going from`` one point (i.e. half boundary) to the other, the surface integral can be understood as ``going from'' one line (i.e. half boundary) to the other.
	
	For example, suppose we have an exact 2-functional, that is it gives zero for all closed surfaces. Then two surfaces that share the same boundary have the same value. Pick a reference point. Pick a family of lines such that they all start from the reference point, all end at different point (covering the whole space) and never form two paths to the same point. For example, in local coordinates, change one coordinate at a time (i.e. first increase the x, then increase the y, then the z, ...). Now pick a scalar function (a (2 - 2)-form) and assign to each line the difference at the boundary (this arbitrary choice is the equivalent of choosing the constant function in the previous case, and effectively maps to the choice of gauge). Given any other line, we can use the family to find two lines to form a closed loop. A closed loop identifies, which can now be given a value based on the 2-functional.
	
	This should be generalizable with the following sketch. Take a set of surfaces $R \subset S^{k-1}$, called references, that:
	\begin{enumerate}
		\item includes the empty surface $\emptyset$
		\item the union of a family of surface is in $R$
		\item subsurface of a surface is in $R$
		\item no two surfaces share the same boundary.
	\end{enumerate}
	(Note: some care needs to be done with the definition for $k=1$) Therefore, for any $\sigma^{k-1} \in S^{k-1}$ we find a unique $R(\sigma^{k-1}) \in R$ such that $\partial R(\sigma^{k-1}) = \partial \sigma^{k-1}$. That is, for any surface we find a reference surface with the same boundary, and together $\partial \sigma^{k-1} \cup R(\sigma^{k-1})$ they form a closed surface. Since $f$ is exact, $f(\sigma^k)$ depends only on the boundary of $\sigma^k$: two surfaces with equal boundary can be joined together to form a surface with no boundary, for which $f$ is zero. Therefore we can define $\hat{f}_{k-1} : S^{k-1} \to \mathbb{R}$ such that $\hat{f}_{k-1}(\sigma^{k-1}) = f_k(\hat{\sigma}^{k})$ where $\hat{\sigma}^{k}$ is any surface such that $\partial \hat{\sigma}^{k} = \sigma^{k-1}$. Now take an exact functional $v \in F_{k-1}$. Define $g \in F_{k-1}$ such that $g(\sigma^{k-1}) = \hat{f}_{k-1}(\sigma^{k-1} \cup R(\sigma^{k-1})) + v(R(\sigma^{k-1}))$. We have $\partial g(\sigma^{k}) = g(\partial \sigma^{k}) = \hat{f}_{k-1}(\partial \sigma^{k} \cup R(\partial \sigma^{k})) + v(R(\partial \sigma^{k}))$. Since $\partial \partial \sigma^k = \emptyset$, $R(\partial \sigma^{k}) = \emptyset$ because that is the only surface in $R$ with an empty boundary. Therefore $\partial g(\sigma^k) = \hat{f}_{k-1}(\partial \sigma^k \cup \emptyset) + v(\emptyset) = \hat{f}_{k-1}(\partial \sigma^k) = f_k(\sigma^k)$. Which means $\partial g = f$.
\end{remark}


\iffalse

\section{Differential forms}

This section needs to show that vectors and differential forms are infinitesimal counterparts of surfaces and functional. 


\begin{defn}
	TODO: Define a \textbf{$k$-vector} $v^k \in V^k$ as an infinitesimal parallelepiped. We note $V^k$ as the set of all vectors of rank k and $V = \bigcup_{k=0}^n V^k$ as the set of all $k$-vectors. 
\end{defn}


\begin{defn}
	The \textbf{wedge product} $\wedge : V^k\times V^j \to V^{k+j}$ returns the $(k+j)$-vector that represents the parallelopiped formed by the sides represented by the given $k$-vector and $j$-vector. 
\end{defn}


\begin{remark}
	Notation for a generic vector. Infinitesimal displacement $dP$ is a vector and can be expressed as: $dP = dx \frac{\partial P}{\partial x^i}$. We can set $e_i = \frac{\partial P}{\partial x^i}$ so $dP = dx^i e_i$.
	
	Every infinitesimal $k$-surface $d\sigma^k$ can be expressed, in terms of the wedge product, as $d\sigma^k= dx^{i_1}dx^{i_2}...dx^{i_k}\frac{\partial P}{\partial x^1} \wedge \frac{\partial P}{\partial x^2} \wedge ... \wedge \frac{\partial P}{\partial x^k} = dx^{i_1}dx^{i_2}...dx^{i_k} e_1 \wedge e_2 \wedge ... \wedge e_k$.
	
	Suppose we have a $k$-surface in terms of $k$ coordinates $s^j$. We will have a differentiable function $x^i = x^i(s^j)$ that maps the parametrization of the $k$-surface into the manifold. At each point $P$, we can write $dx^i = \frac{\partial x^i}{\partial s^j} ds^j$. Therefore we have $d\sigma^k = dx^{i_1}dx^{i_2}...dx^{i_k} e_1 \wedge e_2 \wedge ... \wedge e_k$.
	
	For example:
	\begin{align*}
		x^i &= \{x,y,z\} \\
		s^j &= \{\varphi, \theta\} \\
		x &= \sin \varphi \cos \theta \\ 
		y &= \sin \varphi \sin \theta \\ 
		z &= \cos \varphi \\ 
		d\sigma &= d\varphi d\theta (e_\varphi \wedge e_\theta) \\
		&=d\varphi d\theta \left(\frac{\partial x}{\partial \varphi} e_x + \frac{\partial y}{\partial \varphi} e_y + \frac{\partial z}{\partial \varphi} e_z\right) \wedge \left(\frac{\partial x}{\partial \theta} e_x + \frac{\partial y}{\partial \theta} e_y + \frac{\partial z}{\partial \theta} e_z\right) \\
		&=d\varphi d\theta (
\frac{\partial x}{\partial \varphi} e_x \wedge \frac{\partial x}{\partial \theta} e_x +
\frac{\partial x}{\partial \varphi} e_x \wedge \frac{\partial y}{\partial \theta} e_y +
\frac{\partial x}{\partial \varphi} e_x \wedge \frac{\partial z}{\partial \theta} e_z + \\
&\frac{\partial y}{\partial \varphi} e_y \wedge \frac{\partial x}{\partial \theta} e_x +
\frac{\partial y}{\partial \varphi} e_y \wedge \frac{\partial y}{\partial \theta} e_y +
\frac{\partial y}{\partial \varphi} e_y \wedge \frac{\partial z}{\partial \theta} e_z + \\
&\frac{\partial z}{\partial \varphi} e_z \wedge \frac{\partial x}{\partial \theta} e_x +
\frac{\partial z}{\partial \varphi} e_z \wedge \frac{\partial y}{\partial \theta} e_y + 
\frac{\partial z}{\partial \varphi} e_z \wedge \frac{\partial z}{\partial \theta} e_z ) \\
		&=d\varphi d\theta ((
\frac{\partial x}{\partial \varphi} \frac{\partial y}{\partial \theta} - \frac{\partial y}{\partial \varphi}\frac{\partial x}{\partial \theta}) e_x \wedge e_y + \\
& (\frac{\partial y}{\partial \varphi} \frac{\partial z}{\partial \theta} - \frac{\partial z}{\partial \varphi} \frac{\partial y}{\partial \theta}) e_y \wedge e_z + \\
&\frac{\partial z}{\partial \varphi} \frac{\partial x}{\partial \theta} - \frac{\partial x}{\partial \varphi} \frac{\partial z}{\partial \theta}) e_z \wedge e_x ) \\
		&=d\varphi d\theta ((
\cos \varphi \cos \theta \sin \varphi \cos \theta - \cos \varphi \sin \theta \sin \varphi (-\sin \theta)) e_x \wedge e_y + \\
& (\cos \varphi \sin \theta  0 - (- \sin \varphi) \sin \varphi \cos \theta) e_y \wedge e_z + \\
& - \sin \varphi \sin \varphi (-\sin \theta) - \cos \varphi \cos \theta 0) e_z \wedge e_x ) \\
		&=d\varphi d\theta (\cos \varphi \sin \varphi e_x \wedge e_y +
\sin^2 \varphi \cos \theta e_y \wedge e_z +
\sin^2 \varphi \sin \theta e_z \wedge e_x )
	\end{align*}
\end{remark}

\begin{defn}
	A \textbf{$k$-form} $\omega_k : V^k \to \mathbb{R}$ is a linear function of a vector. We note $\Omega_k$ as the set of all $k$-forms of dimension k and $\Omega = \cup_{k=0}^n\Omega_k$ as the set of all forms. 
\end{defn}

\begin{prop}
	TODO Show that every k-functional has a corresponding k-form, such that for $f_k = \int_{\sigma^k} \omega_k(d\sigma^k)$. In words, a linear functional applied over a k-surface is the same as an integral of a k-form over the infinitesimal parallelepipedes of that k-surface. 
\end{prop}

\fi




