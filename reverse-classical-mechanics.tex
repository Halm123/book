\chapter{Classical mechanics}

The standard view in physics is that classical mechanics is perfectly understood. It has three different but equivalent formulations, the oldest of which, Newtonian mechanics, is based on three laws. Classical mechanics is the theory of point particles that follow those laws. Unfortunately, this view is incorrect.

We will see that the three formulations are not equivalent, in the sense that there are physical systems that are Newtonian but not Hamiltonian and vice-versa. There are also a number of questions that have been left unanswered, such as the precise nature of the Hamiltonian or the Lagrangian, and what exactly the principle of stationary action represents physically. While shedding light on these issues, we will also find that classical mechanics already contains elements that are typically associated with other theories, such as quantum mechanics/field theories (uncertainty principle, anti-particles), thermodynamics/statistical mechanics (thermodynamic and information entropy conservation) or special relativity (energy as the time component of a four-vector). In other words, the common understanding of classical mechanics is quite shallow, and its foundations are, in fact, not separate from the ones of classical statistical mechanics or special relativity.

What reverse physics shows is that the central assumption underneath classical mechanics is that of \textbf{infinitesimal reducibility}: a classical system can be thought of as made of parts, which in turn are made of parts and so on; studying the whole system is equivalent to studying all its infinitesimal parts. This assumption, together with the assumption of \textbf{independence of degrees of freedom}, is what gives us the structure of classical phase space with conjugate variables. The additional assumption of \textbf{determinism and reversibility}, the fact that the description of the system at one time is enough to predict its future or reconstruct its past, leads us to Hamiltonian mechanics. On the other hand, assuming \textbf{kinematic equivalence}, the idea that trajectories in space are enough to reconstruct the state of the system and vice-versa, leads to Newtonian mechanics. The combination of all above assumptions, instead, leads to Lagrangian mechanics and, in particular, to massive particles under (scalar and vector) potential forces.

\section{Formulations of classical mechanics}

In this section we will briefly review the three main formulations of classical mechanics. Our task is not to present them in detail, but rather to provide a brief summary of the equations so that we can proceed with the comparison. In particular, given that different conventions are used across formulations, within the same formulation and among different contexts (e.g. relativity, symplectic geometry), we will want to make the notation homogeneous to allow easier comparisons.

\subsection{Newtonian mechanics}

For all formulations, the system is modeled as a collection of point particles, though we will mostly focus on the single particle case. For a Newtonian system, the state of the system at a particular time $t$ is described by the position $x^i$ and velocity $v^i$ of all its constituents. Each particle has its mass $m$, not necessarily constant in time, and, for each particle, we define kinetic momentum as $\Pi^i = m v^i$.\footnote{We will use the letter $t$ for the time variable, $x$ for position and $v$ for velocity, which is a very common notation in Newtonian mechanics. However, we will keep using the same letters in Lagrangian mechanics as well, instead of $q$ and $\dot{q}$, for consistency. Given that the distinction between kinetic and conjugate momentum is an important one, we will denote $\Pi$ the former and $p$ the latter. The Roman letters $i,j,k,...$ will be used to span the spatial components, while we will use the Greek letters $\alpha, \beta, \gamma, ...$ to span space-time components. Unlike some texts, $x^i$ do not represent Cartesian coordinates, and therefore they should be understood already as generalized coordinates.}

The evolution of our system is given by Newton's second law:\footnote{For derivatives, we will use the shorthand $d_t$ for $\frac{d}{dt}$ and $\partial_{x^i}$ for $\frac{\partial}{\partial x^i}$. }
\begin{equation}\label{rp-cm-NewtonsSecondLaw}
	F^i(x^j, v^k, t) = d_t \Pi^i.
\end{equation}
Mathematically, if the forces $F^i$ are Lipschitz continuous, then the solution $x^i(t)$ is unique. That is, given position and velocity at a given time, we can predict the position and velocity at future times. We will assume a Newtonian system has this property.

An important aspect of Newtonian mechanics is that the equations are not invariant under coordinate transformation. To distinguish between apparent forces (i.e. those dependent on the choice of frame) and the real ones, we assume the existence of inertial frames. In an inertial frame there are no apparent forces, and therefore a free system (i.e. no forces) with constant mass proceeds in a linear uniform motion, or stays still.\footnote{Recall that linear motion simply means that it describes a line in space, while uniform motion means that the speed is constant. Therefore we can have linear non-uniform notion (e.g. an object accelerated along the same direction) or a non-linear uniform motion (e.g. an object going around in a circle at constant speed).}

\subsection{Lagrangian mechanics}

The state for a Lagrangian system is also given by position $x^i$ and velocity $v^i$. The dynamics is specified by a single function $L(x^i, v^j, t)$ called the Lagrangian. For each spatial trajectory $x^i(t)$ we define the action as $\mathcal{A}[x^i(t)] = \int_{t_0}^{t_1} L(x^i(t), d_t x^i(t), t) dt$. The trajectory taken by the system is the one that makes the action stationary:
\begin{equation}
\delta \mathcal{A}[x^i(t)] = \delta \int_{t_0}^{t_1} L\left(x^i(t), d_t x^i(t), t\right) dt=0
\end{equation}
The evolution can equivalently be specified by the Euler-Lagrange equations:
\begin{equation}\label{rp-cm-EulerLagrange}
	\partial_{x^i}L=d_t \partial_{v^i} L.
\end{equation}

Note that not all Lagrangians lead to a unique solution. For example, $L=0$ will give the same action for all trajectories and therefore, strictly speaking, all trajectories are possible. The stationary action leads to a unique solution if and only if the Lagrangian is hyperregular, which means the Hessian matrix $\partial_{v^i}\partial_{v^j} L$ is invertible. Like in the Newtonian case, we will assume Lagrangian systems satisfy this property.

Unlike Newton's second law, both the Lagrangian and the Euler-Lagrange equations are invariant under coordinate transformations. This means that Lagrangian mechanics is particularly suited to study the symmetries of the system.

\subsection{Hamiltonian mechanics}

In Hamiltonian mechanics, the state of the system is given by position $q^i$ and conjugate momentum $p_i$. The dynamics is specified by a single function $H(q^i, p_j, t)$ called the Hamiltonian.\footnote{We use a different symbol for position in Hamiltonian mechanics because, while it is true that $q^i = x^i$, it is also true that $\partial_{q^i} \neq \partial_{x^i}$: the first derivative is taken at constant conjugate momentum while the second is taken at constant velocity. This creates absolute confusion when mixing and comparing Lagrangian and Hamiltonian concepts, which our notation avoids completely.} The evolution is given by Hamilton's equations:
\begin{equation}\label{rp-cm-HamiltonEq}
	\begin{aligned}
		d_t q^i = \partial_{p_i} H \\
		d_t p_i = - \partial_{q^i} H \\
	\end{aligned}
\end{equation}
We will again want these equations to yield a unique solution, which means the Hamiltonian must be at least differentiable, and the derivatives must at least be Lipschitz continuous.

Hamilton's equations are invariant as well. The Hamiltonian itself is a scalar function which is often considered (mistakenly as we'll see later) invariant. This formulation is the most suitable for statistical mechanics as volumes of phase space correctly count the number of possible configurations.

\section{Inequivalence of formulations}

It is often stated in physics books that all three formulations of classical mechanics are equivalent. We will look at this claim in detail, and conclude that this is not the case: there are systems that can be described by one formulation and not another. More precisely, the set of Lagrangian systems is exactly the intersection of Newtonian and Hamiltonian systems.

We will consider two formalisms equivalent if they can be applied to exactly the same systems. That is, Newtonian and Lagrangian mechanics are equivalent if any system that can be described using Newtonian mechanics can also be described by Lagrangian mechanics and vice-versa. In general, in physics great emphasis is put on systems that can indeed be studied by all three, leaving the impression that this is always doable.\footnote{If one asks the average physicist whether Newtonian and Hamiltonian mechanics are equivalent, the answer most of the time will be  enthusiastically positive. If one then asks for the Hamiltonian for a damped harmonic oscillator, the typical reaction is annoyance due to the nonsensical question (damped harmonic oscillators do not conserve energy), followed by a realization and partial retraction of the previous claim. The moral of the story is to never take these claims at face value.} However, just with a cursory glance, we realize that this can't possibly be the case.

The dynamics of a Newtonian system, in fact, is specified by three independently chosen functions of position and velocity, the forces applied to each degree of freedom. On the other hand, the dynamics of Lagrangian and Hamiltonian systems is specified by a single function of position and velocity/momentum, the Lagrangian/Hamiltonian. Intuitively, there are more choices in the dynamics for Newtonian systems than for Lagrangian and Hamiltonian.

Now, the reality is a bit trickier because the mathematical expression of the forces is not enough to fully characterize the physical system. We need to know in which frame we are, what coordinates are being used and the mass of the system, which is potentially a function of time. On the Lagrangian side, note that the Euler-Lagrange equations are homogeneous in $L$. This means that multiplying $L$ by a constant leads to the same solutions, meaning that the same system can be described by more than one Lagrangian. The converse is also true: if one system is half as massive and is subjected to a force half as intense, the resulting Lagrangian is also simply rescaled by a constant factor. Therefore the map between Lagrangians and Lagrangians system is not one-to-one: it is many-to-may. This is why we should never look simply at mathematical structures if we want to fully understand the physics they describe.

Regardless, our task is at the moment much simpler: we only need to show that there are Newtonian systems not expressible by Lagrangian or Hamiltonian mechanics. We can therefore limit ourselves to systems with a specific constant mass $m$ in an inertial frame. Every possible expression of the force is allowed and will lead to a unique expression of the acceleration $a^i=F^i(x^j, v^k, t)/m$ which means a unique set of possible trajectories for each expression of the force. Now, we can think of trying to write a Lagrangian for each of those systems. Assuming that this is possible, the acceleration $a^i=F^i[L]/m$ is going to be some functional of that Lagrangian, which, given the Euler-Lagrange equations \ref{rp-cm-EulerLagrange} must be continuous: for small variation of the Lagrangian we must have a small variation of the equations of motion and therefore of the acceleration. But a continuous surjective map from the space of a single function (i.e. the Lagrangian) to the space of multiple functions (i.e. those that specify the forces) does not exist, and therefore there must be at least one Netwonian system with constant mass expressed in an inertial frame that is not describable using Lagrangian mechanics. The same argument applies for Hamiltonian mechanics, since the dynamics in this case is also described by a single function in the same number of arguments. We therefore reach the following conclusion:
\begin{equation}
	\textrm{Not all Netwonian systems are Lagrangian and/or Hamiltonian.}
\end{equation}

We now want to understand whether all Lagrangian systems are Newtonian. Given what we discussed, we cannot expect to reconstruct the mass and force uniquely from the expression of the Lagrangian. We consider the mass and the frame fixed by the problem, together with the Lagrangian, and therefore we must only see whether we can indeed find a unique expression for the acceleration. From the Euler-Lagrange equations \ref{rp-cm-EulerLagrange} we can write
\begin{equation}
	\begin{aligned}
	\partial_{x^i}L&=d_t \partial_{v^i} L=\partial_{q^j} \partial_{v^i} L \, d_t q^j + \partial_{v^k} \partial_{v^i} L \, d_t v^k = \partial_{q^j} \partial_{v^i} L \, v^j + \partial_{v^k} \partial_{v^i} L \, a^k \\
	\partial_{v^k} &\partial_{v^i} L \, a^k = \partial_{x^i}L - \partial_{q^j} \partial_{v^i} L \, v^j .
	\end{aligned}
\end{equation}
To be able to write the acceleration explicitly, we must be able to invert the Hessian matrix $\partial_{v^k} \partial_{v^i} L$. As we noted before, this is exactly the condition for which the principle of stationary actios leads to a unique solution, and we can better understand why. If it is not invertible at a point, the determinant is zero and therefore one eigenvalue is zero. The corresponding eigenvector corresponds to a direction for which the equation tells us nothing, and therefore a variation of the acceleration in that direction will not change the action. This is why the invertibility of the Hessian is required in order to obtain unique solutions.

What we find, then, is that for any Lagrangian system, which we assume to have a unique solution, we can explicitly write the acceleration as a function of position, velocity and time. Therefore
\begin{equation}
	\textrm{All Lagrangian systems are Newtonian.}
\end{equation}

Now we turn our attention to Hamiltonian mechanics and, similarly, we ask whether we can express the acceleration as a function of position and velocity. We have
\begin{equation}
	\begin{aligned}
		a^i &= d_t v^i = d_t d_t q^i = d_t \partial_{p_i} H = \partial_{q^j} \partial_{p_i} H d_t q^j + \partial_{p_k} \partial_{p_i} H d_t p_k \\
		&= \partial_{q^j} \partial_{p_i} H \partial_{p_j} H - \partial_{p_k} \partial_{p_i} H \partial_{q^k} H.
	\end{aligned}
\end{equation}
This tells us that the acceleration is always an explicit function, but it is, in general, an explicit function of position and momentum, not of position and velocity. To change the expression, we need to be able to write the momentum as a function of position and velocity. Note that Hamilton's equations already give a way to express the velocity in terms of position and momentum, we just need that expression to be invertible, which means the Jacobian must be invertible. We must have:
\begin{equation}
	\left|\partial_{p_i} v^j\right| = \left|\partial_{p_i}\partial_{p_j} H\right| \neq 0 .
\end{equation}
To be able to express momentum as a function of position and velocity, then, we need the Hessian of the Hamiltonian to be invertible (i.e. to have non-zero determinant).

Note that we had no such requirement for the Hamiltonian. For example, $H=0$ leads to equations $d_t q^i = 0$ and $d_t p_i = 0$, which have unique solutions: both position $q^i(t) = k_{q^i}$ and momentum $p_i(t) = k_{p_i}$ are constants of motion. The Hessian, being the zero matrix, is not invertible, and in fact we cannot write momentum as a function of position and velocity: velocity $d_t q^i$ is always zero in all cases while conjugate momentum can be any value $k_{p_i}$. Though this case may not be physically interesting, it is a perfectly valid Hamiltonian system and shows that we should always check the trivial mathematical case. However, let us go through a more physically meaningful case.

\textbf{Photon as a particle}. If we want to treat the photon as a classical particle, we can write the Hamiltonian by expressing the energy as a function of momentum
\begin{equation}
	H=\hbar | \omega| = c \hbar |k| = c |p|.
\end{equation}
If we apply Hamilton's equations, we have
\begin{equation}
	\begin{aligned}
		d_t q^i &= c \frac{p^i}{|p|} \\
		d_t p_i &= 0.
	\end{aligned}
\end{equation}
That is, the norm of the velocity is always $c$, the momentum decides its direction, and the momentum itself does not change in time. This is indeed the motion of a free photon. One can confirm, through tedious calculation, that the determinant of the Hessian is indeed zero, yet it is easier and more physically instructive to see that we cannot reconstruct the momentum from the velocity. Relativistically, all photons travel along the geodesics at the same speed, therefore two photons that differ only by the magnitude of the momentum will travel the same path.

Hamiltonian systems that are also Newtonian, then, need to satisfy this extra condition, so let us give it a name.
\renewcommand{\theassump}{KE}%
\begin{assump}[Kinematic Equivalence]\label{assum_kineq}
	The kinematics of the system is enough to reconstruct its dynamics and vice-versa.
\end{assump}
\renewcommand{\theassump}{\Roman{assump}}%
By kinematics we mean the motion in space and time and by dynamics we mean the state and its time evolution in phase space. We will need to analyze the difference between the two more in detail, but we should first finish our comparison between the different formulations.

Summing up, we find that
\begin{equation}
	\textrm{Not all Hamiltonian systems are Newtonian: only those for which  \ref{assum_kineq} is valid.}
\end{equation}


We now need to compare Lagrangian and Hamiltonian systems. The task is a lot easier because we already have a precise way to connect the two. If we are given a Lagrangian $L$, we define the conjugate momentum $p_i = \partial_{v^i} L$ and the Hamiltonian $H = p_i v^i - L$. If we are given a Hamiltonian $H$, we can define a Lagrangian $L = p_i v^i - H$ and a velocity $v^i = d_t q^i = \partial_{p_i} H$. The only detail that needs to be understood is whether this can be done for all Lagrangian and Hamiltonian systems.

While these expression are always defined, we need to check whether we can change variables; whether we can write the Lagrangian in terms of position and velocity and the Hamiltonian in terms of position and momentum. Going from a Hamiltonian to a Lagrangian, it again means that we can write momentum as a function of position and velocity, and therefore assumption \ref{assum_kineq} must hold. This makes sense: if all Lagrangian systems are Newtonian, and \ref{assum_kineq} was required for a Hamiltonian system to be Newtonian, then it also required for a Hamiltonian system to be Lagrangian. But the connection is stronger: \ref{assum_kineq} is the \emph{only} additional assumption we need to be able to write a Lagrangian given a Hamiltonian.

Going from a Lagrangian to a Hamiltonian, it means that we can write velocity as a function of position and momentum. Note that since we define conjugate momentum as the derivative of the Lagrangian, we can already express momentum as a function of position and velocity, which means we are simply asking that expression to be invertible. This is, again, assumption \ref{assum_kineq}, just in the opposite direction. We must have
\begin{equation}
	0 \neq \left| \partial_{v^i} p_j \right| = \left| \partial_{v_i} \partial_{v_j} L \right|.
\end{equation}
This means that assumption \ref{assum_kineq} is exactly the invertibility of the Hessian, the condition for unique solution of the Lagrangian. All Lagrangian systems that admit unique solutions, then, satisfy assumption \ref{assum_kineq}. In fact, we can see that the Hessian determinants are related
\begin{equation}
	\left| \partial_{v_i} \partial_{v_j} L \right| = \left| \partial_{v^i} p_j \right| = \left| \partial_{p_i} v^j \right|^{-1} = \left|\partial_{p_i}\partial_{p_j} H\right|^{-1}.
\end{equation}
This means that every Lagrangian admits a Hamiltonian, but not every Hamiltonian admits a Lagrangian. Only the Hamiltonian systems for which \ref{assum_kineq} is valid will also be Lagrangian systems, with a guaranteed unique solution given that \ref{assum_kineq} is exactly the assumption needed for that as well. Therefore we conclude that
\begin{equation}
	\textrm{Lagrangian systems are exactly those Hamiltonian systems for which \ref{assum_kineq} is valid.}
\end{equation}

The relationship between the different formulations, then, can be summarized with the following Venn diagram.

TODO: Venn diagram of the different formulations

Lagrangian mechanics is exactly the intersection of Newtonian mechanics and Lagrangian mechanics.

We have found that \ref{assum_kineq} is a constitutive assumption of Lagrangian mechanics, and that it clearly marks which Hamiltonian systems are Newtonian/Lagrangian. By constitutive assumption we mean an assumption that must be taken, either explicitly or implicitly, for a theory to be valid. But what makes a system Hamiltonian and what makes a system Newtonian? Can we find a full set of constitutive assumptions for classical mechanics?

\section{Kinematics vs dynamics}

We have seen the importance between kinematics and dynamics. In this section we will explore this link more deeply and come to the following conclusion: the kinematics of a system is not enough to reconstruct its dynamics. 

Let us first review exactly what the kinematics and dynamics are. Given a system, its kinematics is the description of its motion in space and time. Position, velocity, and acceleration are kinematic variables because they describe the motion. Kinematics is what Galileo studied and started to give a rigorous account of. The dynamics, instead, describes the cause of such motion. Force, mass, momentum, energy are dynamic quantities as they are used to describe why a body moves in a particular way. Dynamics is what Newton introduced and his second law, expressed as $F=ma$, clearly shows the link.

The link between the two concepts seems important given the constitutive role of \ref{assum_kineq} in Lagrangian mechanics. Moreover, while both Newtonian and Hamiltonian mechanics are dynamical theories, in the sense that quantities like force and momentum are intrinsic parts of the respective theories, Lagrangian mechanics seems to be a purely kinematic theory, as it is described only by kinematic variables like position and velocity. Therefore it seems useful to characterize the kinematics-dynamics link as much as possible. Let's analyze a concrete example.

Suppose we are given the following equation:
\begin{equation}\label{rp-cm-frictionEquation}
	m a = - b v .
\end{equation}
The equation is in terms of kinematics variables and, given initial conditions $x_0$ and $v_0$, it admits a unique solution, a unique trajectory.
The solution is
\begin{equation}
	\begin{aligned}
	x(t)&= x_0 + v_0 \frac{m}{b} \left( 1 - e^{-\frac{b}{m}t}\right) \\
	v(t)&= v_0 e^{-\frac{b}{m}t} \\
	a(t)&= - v_0 \frac{b}{m} e^{-\frac{b}{m}t}
	\end{aligned}
\end{equation}
Can we reconstruct the forces acting on this system?

The obvious answer seems to be that the constant $m$ represents the mass of the system and $F = -bv$ the force. This is the case of a particle under linear drag:  the system is subjected to a frictional force that is proportional and opposite to the velocity. If we set the Lagrangian
\begin{equation}\label{rp-cm-frictionLagrangian}
	L = \frac{1}{2} m v^2 e^{\frac{b}{m}t}.
\end{equation}
and apply the Euler-Lagrange equation \ref{rp-cm-EulerLagrange} we have
\begin{equation}
	\begin{aligned}
	\partial_x L &= 0 = d_t \partial_v L = d_t \left(m v e^{\frac{b}{m}t} \right)=mae^{\frac{b}{m}t} + \frac{b}{m} m v e^{\frac{b}{m}t} = e^{\frac{b}{m}t}(ma + bv) \\
	ma &= - bv.
	\end{aligned}
\end{equation}
Therefore we have a Lagrangian for the system. We can also find a Hamiltonian
\begin{equation}\label{rp-cm-kd-momentumHamiltonian}
	\begin{aligned}
	p &= \partial_v L = m v e^{\frac{b}{m}t} \\
		v &= \frac{p}{m} e^{-\frac{b}{m}t} \\
		H &= p v - L = p \frac{p}{m} e^{-\frac{b}{m}t} - \frac{1}{2} m \left( \frac{p}{m} e^{-\frac{b}{m}t} \right)^2 e^{\frac{b}{m}t} = \frac{p^2}{m}  e^{-\frac{b}{m}t} - \frac{1}{2} \frac{p^2}{m}  e^{-\frac{b}{m}t} \\ 
		&=\frac{1}{2} \frac{p^2}{m}  e^{-\frac{b}{m}t}
	\end{aligned}
\end{equation}
and apply Hamilton's equations \ref{rp-cm-HamiltonEq}
\begin{equation}
	\begin{aligned}
		d_t q &= \partial_p H = \frac{p}{m}  e^{-\frac{b}{m}t} \\
		d_t p &= - \partial_q H = 0. 
	\end{aligned}
\end{equation}
The second equation tells us momentum is constant $p_0$. Substituting the constant in the first equation, we have the velocity as a function of time, which we can integrate. We have
\begin{equation}
	\begin{aligned}
	q(t) &= q_0 + \frac{p_0}{b} \left( 1 - e^{-\frac{b}{m}t}\right) \\
	p(t) &= p_0.
	\end{aligned}
\end{equation}

The kinematics works perfectly, but the dynamics seems off. First of all, based on the physics, one would expect the momentum to be decreasing in time
\begin{equation}
	p(t)=m v(t) = m v_0 e^{-\frac{b}{m}t}.
\end{equation}
However, conjugate momentum is a constant of motion. For the energy, we would expect the Hamiltonian to match the kinetic energy
\begin{equation}
	E(t)=\frac{1}{2} m v^2(t) = \frac{1}{2} m v_0^2 e^{-2\frac{b}{m}t}
\end{equation}
but if we express the Hamiltonian in terms of velocity we have
\begin{equation}
	H(t)=\frac{1}{2} \frac{p^2}{m} e^{-\frac{b}{m}t} = \frac{1}{2} \frac{1}{m} \left( m v(t) e^{\frac{b}{m}t} \right)^2 e^{-\frac{b}{m}t}= \frac{1}{2} m v^2(t) e^{\frac{b}{m}t} = \frac{1}{2} m v_0^2 e^{-\frac{b}{m}t}.
\end{equation}
That is, the energy decreases more slowly than it should. This is not good.

Now, it is true that conjugate momentum in not the same as kinetic momentum. But the difference, as we will see much more clearly later, is caused by non-inertial non-Cartesian coordinate systems and/or the presence of vector potential forces.\footnote{The relationship is $p_i = m g_{ij} v^j + q A_i$. This reduces to $p_i = m v^i$ if and only if we are in an inertial frame with Cartesian coordinates (i.e. $g_{ij}=\delta_{ij}$) and no forces $A_i = 0$} We are not at all in that case. Also, note that at time $t=0$ the momentum and the energy do match what we expect, but not after. Therefore imagine a situation where friction is non-negligible only in a particular region. We would expect $p=mv$ to be valid before it enters, but not when it comes out. But wouldn't it come out in another region where we would expect $p=mv$ to work? This is strange. How should we proceed?

As it is typical in reverse physics, we will assume that things work in a reasonable way and that we simply have the wrong connection between physics and math. Recall that we started just with an equation, and we then interpreted $m$ to be the mass of the system. Let's just assume that $m$ is a constant with units of mass and define the actual mass of the system as the ratio between conjugate momentum and velocity. Looking back at \ref{rp-cm-kd-momentumHamiltonian}, we have 
\begin{equation}
	\begin{aligned}
	\hat{m}(t) &= p(t) / v(t) = m e^{\frac{b}{m}t} \\
	p(t) &= mv(t)e^{\frac{b}{m}t} = \hat{m}(t) v(t) \\
	H(t) &= \frac{1}{2} \frac{p^2(t)}{m}  e^{-\frac{b}{m}t} = \frac{1}{2} \frac{p^2(t)}{\hat{m}} = \frac{1}{2} \hat{m}(t) v^2(t) = E(t)
	\end{aligned}
\end{equation}
Now everything actually works perfectly: the relationship between velocity and conjugate momentum is respected, the Hamiltonian matches the kinetic energy. We just have a variable mass system. How and why does this work exactly?

Let us expand Newton's second law for a variable mass system. We have:
\begin{equation}
	\begin{aligned}
		F^i &= d_t (\hat{m}v^i) = d_t \hat{m} \, v^i + \hat{m} a^i \\
		\hat{m} a^i &= F^i - d_t \hat{m} v^i
	\end{aligned}
\end{equation}
In particular, for our one dimensional case, let us set $F=0$ and substitute $\hat{m}$
\begin{equation}
	\begin{aligned}
		m e^{\frac{b}{m}t} a &= 0 - d_t m e^{\frac{b}{m}t} v = -\frac{b}{m} m e^{\frac{b}{m}t} v \\
		ma &= -bv.
	\end{aligned}
\end{equation}
Therefore the same equation, the same kinematics, applies to a variable mass system that increases the mass over time. You can imagine, for example, a body that is absorbing mass from all directions, so that the balance of forces on the body is zero. The body, then, is not slowing down because of friction. It is slowing down because energy and momentum are conserved, and if the mass is increasing, the velocity must be decreasing.

In Newtonian mechanics, we can readily distinguish these two cases because we have to be explicit about forces and masses. In Hamiltonian mechanics things are a bit more difficult because, as we will see later more precisely, conjugate momentum is not exactly kinetic momentum and the Hamiltonian is not exactly energy. Yet, conjugate momentum and the Hamiltonian are not kinematic quantities, they are dynamic quantities and therefore we can see that these would be different in different cases. In Lagrangian mechanics this is even more difficult to see because it looks like a purely kinematic theory, while it is not: the Lagrangian itself is not a purely kinematic entity. As we saw, Lagrangian mechanics implicitly assumes \ref{assum_kineq}, which is a condition on the dynamics as well, and the Lagrangian itself is used to reconstruct conjugate momentum and the Hamiltonian. Moreover, if Lagrangian mechanics were a purely kinematic theory, and told us nothing about forces, energy or momentum, it would not be a complete formulation of classical mechanics.

So we have seen that the same kinematic equation can describe a constant mass dissipative system or variable mass non-dissipative system. Is that it? Not quite. Recall that we mentioned that kinetic and conjugate momentum will differ in non-inertial frames. Note that we implicitly assumed that $x$ and $t$ represented the variable for an inertial observer, in the same way that we originally assumed $m$ was the mass of the system. Could the same equation, then, be describing yet another system but in a non-inertial frame?

Let's compare the motion of a particle traveling at constant velocity in an inertial frame, using $t$ as the time variable, and the motion of a particle decelerating exponentially, using $\hat{t}$ as the time variable
\begin{equation}
	\begin{aligned}
		x(t) &= x_0 + v_0 t \\
		x(\hat{t}) &= x_0 + v_0 \frac{m}{b}\left(1-e^{-\frac{b}{m}\hat{t}}\right)
	\end{aligned}
\end{equation}
Note the striking similarity: we can simply set
\begin{equation}
	t = \frac{m}{b} \left(1-e^{-\frac{b}{m}\hat{t}}\right)
\end{equation}
which clearly takes us to a non-inertial frame since uniform motion is no longer uniform in the new frame.

Let's study how Newtown's second law changes if we make a change of time variable while keeping the position variables unchanged
\begin{equation}
	\begin{aligned}
		\hat{t}&=\hat{t}(t) \\
		F^i &= d_t  (m \, v^i) = d_t  (m \, d_t x^i) = d_t \hat{t} \, d_{\hat{t}}  (m \, d_t \hat{t} \, d_{\hat{t}} x^i).
	\end{aligned}
\end{equation}
If we set
\begin{equation}
	\hat{m} = m \, d_t \hat{t}
\end{equation}
we can express the previous equation in the following form
\begin{equation}
	\begin{aligned}
		F^i &= d_t \hat{t} \, d_{\hat{t}}  (\hat{m} \, d_{\hat{t}} x^i) = d_t \hat{t} \, d_{\hat{t}}  (\hat{m} \, \hat{v}^i) = d_t \hat{t} \hat{F}^i.
	\end{aligned}
\end{equation}
This tells us that the second observer will see an effective mass rescaled exactly by the ratio between the time variables. Note that this is exactly what happens in special relativity: the clock for a boosted observer is dilated by a factor of $\gamma$ which is exactly the factor used in the relativistic mass.\footnote{It may be surprising to see a proto-relativistic effect showing up given that no assumption on space-time has been made. As we will see, these types of connections between different theories come up often in reverse physics.} If $t$ is the time variable for an inertial frame and $t(\hat{t})$ is a non-linear function, the resulting frame will be non-inertial and the observer will see an effective variable mass system.

If we look at our problem this way, the rescaling of the mass, then, is not due to a truly variable mass, but a variable effective mass due to the slowing down of the clock. The body slows down because the non-inertial time is slowing down and the body appears to stop because the clock becomes infinitely slow. While this might sound like a contrived case,\footnote{TODO Reminds of black-hole} these are exactly the types of situation a fully relativistic theory (i.e. one that works for all definitions of time and space variables) needs to take into account.

We can verify that this gives us the correct effective mass
\begin{equation}
	\begin{aligned}
	d_{\hat{t}} t  &=d_{\hat{t}} \left( \frac{m}{b} (1-e^{-\frac{b}{m}\hat{t}}) \right) =\frac{m}{b} d_{\hat{t}} (1-e^{-\frac{b}{m}\hat{t}}) = - \frac{m}{b} d_{\hat{t}} e^{-\frac{b}{m}\hat{t}} = + \frac{m}{b} \frac{b}{m} e^{-\frac{b}{m}\hat{t}} = e^{-\frac{b}{m}\hat{t}} \\
	\hat{m} &= m d_t \hat{t} = m (d_{\hat{t}} t)^{-1} = m e^{\frac{b}{m}\hat{t}}.
	\end{aligned}
\end{equation}
And we can verify that we get the same equation by plugging in the time transformation in Newton's second law with a zero force
\begin{equation}
	\begin{aligned}
		0 &= d_t  (m \, v) = d_t  (m \, d_t x) = d_t \hat{t} \, d_{\hat{t}}  (m \, d_t \hat{t} \, d_{\hat{t}} x) \\ &= e^{\frac{b}{m}\hat{t}} \, d_{\hat{t}}  (m e^{\frac{b}{m}\hat{t}} \, \hat{v}) = e^{\frac{b}{m}\hat{t}} \left( m \frac{b}{m} e^{\frac{b}{m}\hat{t}} \hat{v} + m e^{\frac{b}{m}\hat{t}} \hat{a} \right)  = e^{2\frac{b}{m}\hat{t}} \left( b \hat{v} + m \hat{a} \right) \\
		m \hat{a} &= - b \hat{v}.
	\end{aligned}
\end{equation}
Note that the expressions for momentum and energy will match the previous case because the system in the non-inertial frame looks like a variable mass system.

This last case highlights a more subtle issue. In the two previous cases we were in the same inertial frame, we saw the same trajectory, the same kinematics, but we couldn't tell whether we were looking at a fixed mass system under linear drag or a variable mass system: we couldn't tell the dynamics. Now, we have the same system, a constant mass particle under no forces, described in two different frames, one inertial and one not. The motion of the system will naturally have different representations in the different frames, but this does not mean the motion or the causes of motion are different: it's the same object. Therefore we have the same kinematics even though we have different expressions for the same trajectory. The expression $x(t)$, then, is not enough to define the kinematics if we do not know exactly what $x$ and $t$ represent physically, if the frame is not given.

While typically one proceeds by defining the frame first and then the dynamics (i.e. the forces acting on the system), here we have followed a different approach: we first defined the dynamics (i.e. constant mass system under no forces) and then found the frame that matches the given kinematics (i.e. the trajectory or the relationship between velocity and acceleration). Given that Lagrangian and Hamiltonian mechanics are frame invariant, an intrinsic characterization of the system itself is exactly what we should be looking for. Saying, for example, that a system is subjected to no forces or to a linear drag is not frame invariant because forces are not frame invariant.

It is clear that the type of apparent variable mass due to non-inertial frames is unavoidable if we want to have a consistent theory with invariant laws. Therefore both Lagrangian and Hamiltonian mechanics must include these cases. However, it is not exactly clear what to do for true variable mass systems. From a cursory look, it would seem that everything is fine and there is no harm in including them. Yet again, from a cursory look we seemed to have a Lagrangian for a particle under linear drag. As we will see later, there are implicit connections between Lagrangian/Hamiltonian mechanics on one side and thermodynamics, statistical mechanics and special relativity on the other. Given that it is not clear to us whether these connections hold or not,\footnote{For example, areas of phase space are connected to entropy. Does this connection hold with a variable mass system?} we will concentrate on the constant mass case from now on.

Let's recap what we learned. The biggest point is that we can't simply look at the kinematics and understand the causes of motion. The different formulations have different ways to relate the dynamics and the kinematics. Newtonian mechanics is the most clear about the dynamics as it makes us clearly spell out what is going on. This, however, comes at a cost: the equations are not coordinate invariant. The second law, in fact, is valid only for inertial frames with Cartesian coordinates. It is only in these frames, in fact, that a body will proceed in uniform motion if no forces are applied to it. If we are in polar coordinates, for example, the trajectory expressed in radius $\rho$ and angle $\theta$ will not be linear. Even the notion of force is, if one looks closely, a bit ambiguous. In principle, we want to write both the second law $F=ma$ and the expression for work $dW = F dx$. If $dW$ is invariant under change of position variables, the force should be a covector and therefore $dW = F_i dx^i$. But since the acceleration $a$ will change like a vector, we also have $F^i = m a^i$. The notion of force in the second law and in the infinitesimal work are slightly different, and they coincide only if we are in an inertial frame and Cartesian coordinates.

TODO equations are invariant or independent?

On the other side, Hamiltonian and Lagrangian mechanics are coordinate independent: the laws remain the same if we change position variables. This makes them more useful in many contexts. Lagrangian mechanics is more useful when trying to study the symmetries of the system. Hamiltonian mechanics is more useful for statistical mechanics and to better separate degrees of freedom. However, this comes at a price. Hamiltonian and Lagrangian mechanics apply in fewer cases than Newtonian mechanics. As we saw, linear drag may look like it has a valid Hamiltonian/Lagrangian, but it doesn't. For quadratic drag or friction due to normal force, one cannot find a suitable trick, and is forced to use Rayleigh’s dissipation functions which modify the Euler-Lagrange equations. This is not a coincidence: while Newtonian mechanics links kinematics and dynamics by choosing a particular frame, Hamiltonian and Lagrangian mechanics do so by fixing a type of system. It is the implicit knowledge of the type of system that allows us to reconstruct the dynamics just by looking at the kinematics in an unknown frame. What we need to understand, then, is what exactly is this restriction.

\section{Reversing Hamiltonian mechanics}

We now turn our attention to Hamiltonian mechanics and try to understand exactly what types of systems it focuses on. We will find twelve equivalent formulations of Hamiltonian mechanics that link ideas from vector calculus, differential geometry, statistical mechanics, thermodynamics, information theory and plain statistics. The overall result is that Hamiltonian mechanics focuses on systems that are assumed to be deterministic and reversible. We will see how the physical significance of that assumption differs from mathematically naive characterizations.

To simplify our discussion, we will first concentrate on a single degree of freedom. The first characterization of Hamiltonian mechanics is naturally in terms of the equations
\begin{equation}\label{rp-cm-hsd-condEquations}
	\tag{HM-1S}
	\begin{aligned}
		d_t q &= \partial_p H \\
		d_t p &= - \partial_q H.
	\end{aligned}
\end{equation}
We will want to treat phase space as a generic two-dimensional space (i.e. manifold), like we would for a plane in physical space. Since the term coordinates is a bit too generic, we will refer to the collection of position and momentum as state variables and will note them as $\xi^a = [q, p]$. We can now define the displacement field
\begin{equation}\label{rp-cm-displacement1d}
	S^a = d_t \xi^a = [d_t q, d_t p]
\end{equation}
which is a vector field that defines the evolution of the system in time. Hamilton's equations, then, can be expressed as
\begin{equation}\label{rp-cm-hsd-displacementCurl}
	\begin{aligned}
		S^q &= \partial_p H \\
		S^p &= - \partial_q H.
	\end{aligned}
\end{equation}

To bring out the geometric meaning of the equations, we introduce the matrix
\begin{equation}\label{rp-cm-symplectic1d}
	\tag{SF-1}
	\omega_{ab} = \left[\begin{array}{cc}
		\omega_{qq} & \omega_{qp} \\
		\omega_{pq} & \omega_{pp} 
	\end{array} \right]= \left[\begin{array}{cc}
		0 & 1 \\
		-1 & 0 
	\end{array} \right]
\end{equation}
which rotates a vector by a right angle.\footnote{The notion of angle is technically ill-defined in phase space, but this slight imprecision makes it easier to get the point across.} That is, if $v^a = [v^q, v^p]$, then $v_a = v^b \omega_{ba}  = [-v^p, v^q]$.\footnote{The notation is purposely similar to how indexes are raised and lowered in general relativity by the metric tensor $g_{\alpha\beta}$, since $\omega_{ab}$ plays a similar geometric role in phase space. One should be careful, however, that $\omega_{ab}$ is not symmetric, so it matters which side is contracted. In terms of symplectic geometry, the rotated displacement field $S_a$ corresponds to the interior product of the displacement field with the symplectic form, usually noted as $\iota_S \omega$ or $S \lrcorner \, \omega$.} We can rewrite equation \ref{rp-cm-hsd-condEquations} as
\begin{equation}\label{rp-cm-hsd-condGeneralizedEquations}
	\tag{HM-1G}
	\begin{aligned}
		S_a = S^b \omega_{ba} &= \partial_a H 
	\end{aligned}
\end{equation}
which tells us that the displacement field is the gradient of the Hamiltonian rotated by a right angle. Since the gradient is the only perpendicular direction to the regions at constant energy, a right angle rotation will give us a tangent to those regions, making it geometrically evident that the value of the Hamiltonian is a constant of motion. (TODO: add picture and consider reviewing the text) Condition \ref{rp-cm-hsd-condGeneralizedEquations} is just a re-expression of \ref{rp-cm-hsd-condEquations}. Though it is useful, we want to find different mathematical conditions which turn out to be equivalent to the equations.

We start by noting that the displacement field as expressed by \ref{rp-cm-hsd-displacementCurl} looks very similar to a curl of $H$, except that it is a two dimensional version. In vector calculus, a vector field is the curl of another field if and only if its divergence is zero.\footnote{We will leave for now topological requirements as they would be a distraction from the overall point.} This holds here as well. First, we can verify that
\begin{equation}
	\partial_a S^a = \partial_q S^q + \partial_p S^p = \partial_q \partial_p H - \partial_p \partial_q H = 0.
\end{equation}
Geometrically, this means that the flow of $S^a$ through a closed region $\oint \left( S^q dp - S^p dq \right) = 0$ is always zero. Note that, since we are in a two dimensional space, a hyper-surface has dimension $n-1 = 2-1 = 1$ and therefore hyper-surfaces are lines. Therefore we have 
\begin{equation}
	\oint \left( S^q dp - S^p dq \right) = \oint \left( \partial_p H dp + \partial_q H dq \right) = \oint dH = 0.
\end{equation}
That is, the flow of the displacement field is the line integral of the gradient of $H$, which is zero over a closed curve.

Conversely, we can see that each divergenceless field in two dimensions admits a stream function $H$ that satisfies \ref{rp-cm-hsd-condEquations}. Geometrically, we can construct $H$ in the following way. Take a reference point $O$ and assign $H(O) = 0$. For any other point $P$, consider the flow of $S$ through any two lines that connect $O$ and $P$. Given that the flow through the region contoured by those lines must be zero, the flow through each line must be equal. Therefore the flow through a line that connects $O$ and $P$ only depends on the points, it is path independent. We can assign $H(P) = \int_{OP} \left( S^q dp - S^p dq \right)$. If we expand the differential of $H$ we have
\begin{equation}
	dH = \partial_q H dq + \partial_p H dp = - S^p dq + S^q dp.
\end{equation}
If we equate the components, we recover \ref{rp-cm-hsd-condEquations}. Geometrically, at least for the one dimensional degree of freedom, we can understand the difference of the Hamiltonian between two points as the flow of the displacement field between them.

We conclude that the following condition
\begin{equation}\label{rp-cm-hsd-condDivergenceDisplacement}
	\tag{HM-2}
	\text{The displacement field is divergenceless: } \; \partial_a S^a = 0
\end{equation}
is equivalent to \ref{rp-cm-hsd-condEquations}. Unlike \ref{rp-cm-hsd-condGeneralizedEquations}, this is a truly different mathematical condition.

Having looked at the flow through a region, we turn our attention to how regions themselves are transported by the evolution. Liouville's theorem states that volumes of phase space are preserved during Hamiltonian evolution, which in our case will be areas over the $q-p$ plane. To see this, let us review the transformation rules for infinitesimal volumes:
\begin{equation}\label{rp-cm-volumeTransformation1d}
	\begin{aligned}
		\hat{\xi}^a &= \hat{\xi}^a(\xi^b) \\
		d\hat{\xi}^a &= \partial_b \hat{\xi}^a d\xi^b \\
		d\hat{\xi}^1 \cdots d\hat{\xi}^n &= \left| \partial_b \hat{\xi}^a \right| d\xi^1 \cdots d\xi^n \\
		d\hat{q} d\hat{p} &= \left|\begin{array}{ c c }
			\partial_q \hat{q} & \partial_p \hat{q} \\
			\partial_q \hat{p} & \partial_p \hat{p} \\
		\end{array} \right| dq dp \\
	\end{aligned}	
\end{equation}

This tells us that, mathematically, a transformation is volume preserving if the determinant of the Jacobian $\partial_b \hat{\xi}^a$ is unitary. If $\hat{q}$ and $\hat{p}$ represent the evolution of $q$ and $p$ after an infinitesimal time step $\delta t$, we have
\begin{equation}
	\begin{aligned}
	\hat{q} &= q + S^q \delta t \\ 
	\hat{p} &= p + S^p \delta t \\ 
	\partial_b \hat{\xi}^a &= \left[\begin{array}{ c c }
		1 + \partial_q S^q \delta t & \partial_p S^q \delta t \\
		\partial_q S^p \delta t & 1 + \partial_p S^p \delta t \\
	\end{array} \right] \\
	\left|\partial_b \hat{\xi}^a\right| &= (1 + \partial_q S^q \delta t) (1 + \partial_p S^p \delta t) - \partial_p S^q \, \partial_q S^p \, \delta t^2  = 1 + \left(\partial_q S^q + \partial_p S^p \right) \delta t + O(\delta t^2). 
	\end{aligned}
\end{equation}
Note that the first order term is proportional to the divergence of the displacement field, therefore the Jacobian determinant is equal to one if and only if the displacement is divergenceless. In other words, condition
\begin{equation}\label{rp-cm-hsd-condUnitaryJacobian}
	\tag{HM-3}
	\text{The Jacobian of time evolution is unitary: } \; \left|\partial_b \hat{\xi}^a\right|=1
\end{equation}
and condition
\begin{equation}\label{rp-cm-hsd-condConservedVolume}
	\tag{HM-4}
	\text{Volumes are conserved through the evolution: } \; d\hat{\xi}^n = d\xi^n.
\end{equation}
are equivalent to \ref{rp-cm-hsd-condDivergenceDisplacement}. We have found a third and a fourth way to characterize Hamiltonian evolution.

While condition \ref{rp-cm-hsd-condConservedVolume} is expressed in terms of areas, similar considerations will work for densities because a density is a quantity divided by an infinitesimal area. In fact densities
\begin{equation}\label{rp-cm-densityTransformation1d}
	\begin{aligned}
		\left| \partial_b \hat{\xi}^a \right| \hat{\rho}(\hat{\xi}^a) &= \rho(\xi^b).
	\end{aligned}	
\end{equation}
transform in an equal and opposite way with respect to areas (i.e. the Jacobian determinant is on the other side of the equality). The unitarity of the Jacobian determinant, then, is equivalent to requiring that the density at an initial state is always equal to the density at the corresponding final state. Both areas and densities are transported unchanged by Hamiltonian evolution. Therefore
\begin{equation}\label{rp-cm-hsd-condConservedDensity}
	\tag{HM-5}
	\text{Densities are conserved through the evolution: } \; \hat{\rho}(\hat{\xi}^a) = \rho(\xi^b).
\end{equation}
is yet another equivalent characterization.

To get a yet different perspective, we can reframe these arguments in terms of $\omega_{ab}$ and $S_a$. Given two vectors $v^a$ and $w^a$, the area of the parallelogram they form is $v^q w^p - v^p w^q$. This can be rewritten as $v^a \omega_{ab} w^b$, which means we can think of $\omega_{ab}$ as a tensor that, given two vectors, returns the area of the parallelogram they form.\footnote{More properly, it is a two-form.} If we denote $\hat{v}^a = \partial_b \hat{\xi}^a \, v^b$ and $\hat{w}^a = \partial_b \hat{\xi}^a \, w^b$ the transformed vectors, the invariance of the area can be written as
\begin{equation}
	v^a \omega_{ab} w^b = \hat{v}^c \omega_{cd} \hat{w}^d.
\end{equation}
Since
\begin{equation}
	\hat{v}^c \omega_{cd} \hat{w}^d = v^a \, \partial_a \hat{\xi}^c \omega_{cd} \, \partial_b \hat{\xi}^d \, w^b = v^a \, \hat{\omega}_{ab} w^b
\end{equation}
the previous equivalence means that $\omega_{ab} = \hat{\omega}_{ab}$, that is $\omega_{ab}$ remains unchanged. In other words, preserving the area for all possible pairs of vectors is the same as preserving the tensor $\omega_{ab}$ that returns the areas. We now see that $\omega_{ab}$ plays such an important geometric role that
\begin{equation}\label{rp-cm-hsd-condConservedSymplectic}
	\tag{HM-6}
	\text{The evolution leaves $\omega_{ab}$ invariant: } \; \hat{\omega}_{ab} = \omega_{ab}.
\end{equation}
is yet another equivalent characterization of Hamiltonian mechanics.


It is useful to look more closely at the definition of the Poisson bracket
\begin{equation}
	\{f, g\} = \partial_q f \, \partial_p g - \partial_p f \, \partial_q g = \left|\begin{array}{ c c }
		\partial_q f & \partial_p f \\
		\partial_q g & \partial_p g \\
	\end{array} \right|.
\end{equation}
For a single degree of freedom, the Poisson bracket coincides with the Jacobian determinant, where $f$ and $g$ are the two new variables. It essentially tells us how the volume changes if we change state variable from $[q, p]$ to $[f, g]$. Canonical coordinates and transformations, then, are those that do not change the units of area. The Poisson bracket can be expressed\footnote{To see how our definitions and notation map to that used in differential geometry, let us define $\partial^a H = \omega^{ab} \partial_a H$. Note that $\partial^a H$ corresponds to the Hamiltonian vector field of $H$ usually noted $X_H$. The Poisson bracket is usually defined as $\omega(X_f, X_g)$. In our notation this becomes $\partial^a f \omega_{ab} \partial^b g = \omega^{ac} \partial_c f \omega_{ab} \omega^{bd} \partial_d g = \omega^{ac} \partial_c f \delta_a^d \partial_d g = \omega^{ac} \partial_c f \partial_a g$. One can see how the notation mimics the Einstein notation of general relativity and avoids the introduction of ad-hoc symbols.} as
\begin{equation}
	\{f, g\} = - \partial_a f \omega^{ab} \partial_b g = \partial_b g \omega^{ba} \partial_a f
\end{equation}
where 
\begin{equation}
	\omega^{ab} = \left[\begin{array}{cc}
		\omega^{qq} & \omega^{qp} \\
		\omega^{pq} & \omega^{pp} 
	\end{array} \right]= \left[\begin{array}{cc}
		0 & -1 \\
		1 & 0 
	\end{array} \right]
\end{equation}
is the inverse of $\omega_{ab}$. The invariance of the Poisson brackets is equivalent to the invariance of the inverse of $\omega_{ab}$, which is equivalent to \ref{rp-cm-hsd-condConservedSymplectic}. Therefore
\begin{equation}\label{rp-cm-hsd-condConservedPoisson}
	\tag{HM-7}
	\text{The evolution leaves the Poisson brackets invariant}
\end{equation}
is yet another equivalent characterization. So, again, we see how $\omega_{ab}$ plays a fundamental geometrical role.

We can also write rewrite the flow of the displacement field
\begin{equation}
	\int \left( S^q dp - S^p dq \right) = \int S^a \omega_{ab} d\xi^b = \int S_b d\xi^b
\end{equation}
as the line integral of the rotated displacement field $S_a$. We can do that because in two dimension the flow through a boundary is effectively a line integral along the boundary with the field rotated 90 degrees. This means that the following condition
\begin{equation}\label{rp-cm-hsd-condCurlRotatedDisplacement}
	\tag{HM-8}
	\text{The rotated displacement field is curl free: } \; \partial_a S_b - \partial_b S_a = 0
\end{equation}
is equivalent to condition \ref{rp-cm-hsd-condDivergenceDisplacement}.\footnote{Those familiar with relativistic electromagnetism will recognize the expression $\partial_a S_b - \partial_b S_a$ as the generalization of the curl. More properly, it is the exterior derivative applied to a one-form.} In fact, we can read equation \ref{rp-cm-hsd-condGeneralizedEquations} as saying that the rotated displacement field is the gradient of the scalar potential $H$.

We can see that we have found plenty of alternative characterizations of Hamilton's equations \ref{rp-cm-hsd-condEquations} (or \ref{rp-cm-hsd-condGeneralizedEquations}). Conditions  \ref{rp-cm-hsd-condDivergenceDisplacement}, \ref{rp-cm-hsd-condUnitaryJacobian}, \ref{rp-cm-hsd-condConservedVolume} and \ref{rp-cm-hsd-condConservedDensity} relate more directly to the displacement field $S^a$, while conditions \ref{rp-cm-hsd-condConservedSymplectic}, \ref{rp-cm-hsd-condConservedPoisson} and \ref{rp-cm-hsd-condCurlRotatedDisplacement} relate more directly to $\omega_{ab}$ and the rotated displacement field $S_a$. Nonetheless, they are all in terms of the mathematical description. While these are useful, the final goal of reverse physics is to find physical assumptions, not just equivalent mathematical definitions. So it is time to step back and try to understand what the math is really about.

Let us first reflect on what we just found out: the defining characteristic of Hamiltonian mechanics is not the transport of points, but the transport of areas and densities. If classical Hamiltonian mechanics were really about and only about point particles, there would be no reason for it to be characterized by \ref{rp-cm-hsd-condDivergenceDisplacement}, \ref{rp-cm-hsd-condConservedVolume} or \ref{rp-cm-hsd-condConservedDensity}. In fact, there would be no reason for the equations of motion \ref{rp-cm-hsd-condEquations} to be differentiable. Differentiable equations are exactly needed if we need to define the Jacobian, the transport of areas, or of densities defined on those areas. Classical point particles, then, are more aptly conceived not as points, but as infinitesimal regions of phase space, as distributions so peaked that only the mean value is important.

This, in retrospect, matches how classical mechanics is used in practice: planets, cannonballs, pendulums, beads on a wire, all the objects we study with classical mechanics are not point-like objects. They can be considered point-like if their size is negligible compared to the scale of the problem. If the distance between two celestial bodies is smaller than the sum of their radii, the point particle approximation clearly fails. This is also consistent with fluid dynamics and continuous mechanics, where we are literally studying the motion of infinitesimal parts of a material. It is interesting to see echoes of these considerations present in the mathematics.\footnote{We will want to investigate this link in more detail later.}

If we look at physics more broadly, we realize that in statistical mechanics we already have a physical interpretation for volumes of regions in phase space: they represent the number of states. Hamiltonian mechanics, then, maps regions while preserving the number of states. This means that, for each initial state there is one and only one final state, which leads to the following condition:
\begin{equation}\label{rp-cm-hsd-condDetRev}
	\tag{HM-9}
	\text{The evolution is deterministic and reversible.}	
\end{equation}
Note that by reversible here we mean that given the final state we can reconstruct the initial state. Given that areas measure the number of states, \ref{rp-cm-hsd-condDetRev} is equivalent to \ref{rp-cm-hsd-condConservedVolume}, which means this is another characterization of Hamiltonian mechanics. We can also see a connection to \ref{rp-cm-hsd-condConservedDensity}. If we assign a density to an initial state, and we claim that all and only the elements that start in that initial state will end in a particular final state, we will expect the density of the corresponding final state to match. That is, if the evolution is deterministic and reversible, it may shuffle around a distribution, but it will never be able to spread it or concentrate it.

This makes us understand, at a conceptual level, why a dissipative system, like a particle under linear drag, is not a Hamiltonian system. A dissipative system will have an attractor: a point or a region to which the system will tend given enough time. This means that, in time, the area around the attractor must shrink, the density will concentrate over the attractor, but this is exactly what Hamiltonian systems cannot do. Therefore Hamiltonian systems cannot have attractors, they cannot be dissipative. By the same argument, they can't have unstable points or regions from which the system always goes away.

What may be confusing is that the motion of a particle under linear drag may seem reversible, in the sense that we are able to, given the final position and momentum, reconstruct the initial values. Mathematically, it maps points one-to-one and would seem to satisfy \ref{rp-cm-hsd-condDetRev}, even though it is not a Hamiltonian system. This is a perfect example of how focusing on just the points leads to the wrong physical intuition. Physically, we would say that a one meter range of position allows for more configurations than a one centimeter range, even though mathematically they have the same number of points. If we understand that states are infinitesimal areas of phase space, we can see that a dissipative system, though it does map the center points of infinitesimal areas one-to-one, it does not map the full infinitesimal area one-to-one. In this sense dissipative systems fail to be reversible.

Let that sink in: we found that, if the system is deterministic and reversible, it admits a Hamiltonian, a notion of energy, and that energy is conserved over time. This may seem like a surprising and unexpected result. In retrospect, we can make an argument for it based on familiar physics considerations. If a system is deterministic and reversible it means that its evolution only depends on the state of the system itself. This means that it does not depend on the state of anything else. A system whose evolution does not depend on anything else is an isolated system. Therefore a deterministic and reversible system is isolated, and from thermodynamics we know that an isolated system conserves energy. It should not be surprising, then, that a deterministic and reversible system conserves energy. However, we found that not only does it conserve energy, it defines it. Therefore this link between mechanics and thermodynamics is actually deeper than we may think at first, and we should explore it further.

The idea that a dissipative system is not reversible sounds true on thermodynamic grounds. But thermodynamic reversibility is not the ability to reconstruct the initial state, but rather the existence of a process that can undo the change. Alternatively, a process is  thermodynamically reversible if it conserves thermodynamic entropy, which is a more precise characterization.\footnote{The actual existence of a reverse process is not something that can always be guaranteed.} We should not, then, confuse the two notions of reversibility, but we can easily show their relationship. The fundamental postulate of statistical mechanics tells us that the thermodynamic entropy $S = k_B \log W$ is the logarithm of the count of states, which corresponds to volume in phase space. Since the logarithm is a bijective function, conservation of areas of phase space is equivalent to conservation of entropy. Therefore
\begin{equation}\label{rp-cm-hsd-condThermoRev}
	\tag{HM-10}
	\text{The evolution is deterministic and thermodynamically reversible}	
\end{equation}
is yet another characterization of Hamiltonian mechanics.

There is another type of entropy that is also fundamental in both statistical mechanics and information theory: the Gibbs/Shannon entropy $I[\rho(\xi^a)]=-\int \rho \log \rho \, d\xi^1 \cdots d\xi^n$ which is defined for each distribution $\rho(\xi^a)$. Recalling the transformation rules for both volumes \ref{rp-cm-volumeTransformation1d} and densities \ref{rp-cm-densityTransformation1d}, we have
\begin{equation}
	\begin{aligned}
	I[\rho(\xi^a)] &= - \int \rho(\xi^a) \log \rho(\xi^a) \, d\xi^1 \cdots d\xi^n \\
&= - \int  \hat{\rho}(\hat{\xi}^b) \left| \partial_a \hat{\xi^b} \right| \log \left( \hat{\rho}(\hat{\xi}^b) \left| \partial_a \hat{\xi^b} \right| \right) \, d\xi^1 \cdots d\xi^n \\
&= - \int \hat{\rho}(\hat{\xi}^b) \log \left( \hat{\rho}(\hat{\xi}^b) \left| \partial_a \hat{\xi^b} \right| \right) \, d\hat{\xi}^1 \cdots d\hat{\xi}^n \\
&= - \int \hat{\rho}(\hat{\xi}^b) \log \hat{\rho}(\hat{\xi}^b) \, d\hat{\xi}^1 \cdots d\hat{\xi}^n - \int \hat{\rho}(\hat{\xi}^b) \log \left| \partial_a \hat{\xi^b} \right| \, d\hat{\xi}^1 \cdots d\hat{\xi}^n \\
&= I[\hat{\rho}(\hat{\xi}^b)] - \int \hat{\rho}(\hat{\xi}^b) \log \left| \partial_a \hat{\xi^b} \right| \, d\hat{\xi}^1 \cdots d\hat{\xi}^n.
	\end{aligned}
\end{equation}
Information entropy, then, remains constant if and only if the logarithm of the Jacobian determinant is zero, which means the Jacobian determinant is one. Therefore
\begin{equation}\label{rp-cm-hsd-condInformation}
	\tag{HM-11}
	\text{The evolution conserves information entropy}	
\end{equation}
is equivalent to \ref{rp-cm-hsd-condConservedVolume} and is yet another characterization of Hamiltonian mechanics.

The fact that determinism and reversibility is equivalent to conservation of information entropy should not be, in retrospect, surprising. Given a distribution, its information entropy quantifies the average amount of information needed to specify a particular element chosen according to that distribution. If the evolution is deterministic and reversible, giving the initial state is equivalent to giving the final state and therefore the information to describe one or the other must be the same. Determinism and reversibility, then, can be understood as the informational equivalence between past and future descriptions.

Lastly, given that entropy is often associated with uncertainty, it may be useful to understand how Hamiltonian evolution affects uncertainty. Given a multivariate distribution, the uncertainty is characterized by the covariance matrix
\begin{equation}
	cov(\xi^a, \xi^b) = \left[\begin{array}{ c c }
		\sigma^2_q & cov_{q, p} \\
		cov_{p, q} & \sigma^2_p \\
	\end{array} \right].
\end{equation}
The determinant of the covariance matrix gives us a coordinate independent quantity to characterize the uncertainty. If the distribution is narrow enough, we can use the linearized transformation to see how the uncertainty evolves after an infinitesimal time step $\delta t$. We have
\begin{equation}
	\left| cov(\hat{\xi}^c, \hat{\xi}^d) \right| = \left| \partial_a \hat{\xi}^c  \, cov(\xi^a, \xi^b) \, \partial_b \hat{\xi}^d  \right| = \left| \partial_a \hat{\xi}^c \right| \left| cov(\xi^a, \xi^b) \right| \left| \partial_b \hat{\xi}^d  \right|,
\end{equation}
which means the uncertainty remains unchanged if and only if the Jacobian is unitary. So
\begin{equation}\label{rp-cm-hsd-condUncertainty}
	\tag{HM-12}
	\text{The evolution conserves the uncertainty of peaked distributions}	
\end{equation}
is equivalent to \ref{rp-cm-hsd-condConservedVolume} and is another characterization of Hamiltonian mechanics.

This connection gives us yet another insight on the nature of determinism and reversibility in physics. Given that all physically meaningful descriptions are finite precision, a system is deterministic and reversible in a physically meaningful sense if and only if the past/future descriptions can be reconstructed/predicted at the same level of precision. This gives us another perspective as to why areas and densities must be conserved.

We have found twelve equivalent characterizations that link Hamiltonian mechanics, vector calculus, differential geometry, statistical mechanics, thermodynamics, information theory and plain statistics. Though we only talked about the case of a single degree of freedom, it gives us a much better idea of what systems Hamiltonian mechanics is supposed to describe, those that satisfy the following
\renewcommand{\theassump}{DR}
\begin{assump}[Determinism and Reversibility]\label{assum_detrev}
	The system undergoes deterministic and reversible evolution.
\end{assump}
\renewcommand{\theassump}{\Roman{assump}}
We can see how this concept is implemented mathematically: it is not simply a one-to-one map between points. Classical particles should be more properly thought of as infinitesimal regions of phase space. Conceptually, the count of states, the thermodynamic entropy and information entropy are all conserved, and are all equivalent characterizations of determinism and reversibility. In terms of physical measurement, past and future states are given at the same level of uncertainty. But the most important lesson is that the foundations of classical mechanics are not disconnected from the foundations of all other disciplines we encountered. A full understanding of classical mechanics means understanding those connections as well.

\section{Multiple degrees of freedom}

We have seen how \ref{assum_detrev} is a constitutive assumption for Hamiltonian mechanics, and in fact is equivalent to Hamiltonian mechanics for one degree of freedom. We now turn our attention to the general case, and we will find that \ref{assum_detrev}, by itself, is not enough to recover the equations. We will need an additional assumption, that of the independence of degrees of freedom.

First, let's take Hamilton's equations for multiple degrees of freedom
\begin{equation}\label{rp-cm-hmd-condEquations}
	\tag{HM-1M}
	\begin{aligned}
		d_t q^i &= \partial_{p_i} H \\
		d_t p_i &= - \partial_{q^i} H
	\end{aligned}
\end{equation}
and re-express them in terms of generalized state variables. These will be noted as $\xi^a = [q^i, p_i]$ and will span a $2n$-dimensional space (i.e. manifold). The displacement field will be
\begin{equation}\label{rp-cm-displacementNd}
	S^a = d_t \xi^a = [d_t q^i, d_t p_i]
\end{equation}
which again is the vector field that defines the evolution of the system in time. Hamilton's equations, then, can be expressed as
\begin{equation}
	\begin{aligned}
		S^{q^i} &= \partial_{p_i} H \\
		S^{p_i} &= - \partial_{q^i} H.
	\end{aligned}
\end{equation}

Similarly to the previous case, let's introduce the following matrix
\begin{equation}\label{rp-cm-hmd-symplecticForm}
	\tag{SF-N}
	\omega_{ab} = \left[\begin{array}{cc}
		\omega_{q^i q^j} & \omega_{q^i p_j} \\
		\omega_{p_i q^j} & \omega_{p_i p_j} 
	\end{array} \right]= \left[\begin{array}{cc}
		0 & I_n \\
		- I_n & 0 
	\end{array} \right] = \left[\begin{array}{cc}
	0 & 1 \\
	-1 & 0 
\end{array} \right] \otimes I_n
\end{equation}
which performs a 90 degree rotation within each degree of freedom, switching the component between position and momentum. That is, if $v^a = [v^{q^i}, v^{p_i}]$, then $v_a = v^b \omega_{ba}  = [-v^{p_i}, v^{q^i}]$.\footnote{For those versed in symplectic geometry, $v^a \omega_{ab}$ are the components of the one-form $\omega(v, \cdot)$. However, we are not going to call it a one-form as that assumes that the whole object is a map from a vector field to a scalar field, and we do not know whether that is the correct physical understanding. In other words, we want simply to understand what the quantities are doing without being tied, as much as possible, to a particular way to frame it. Full reverse engineering of differential geometry will be done in a later chapter, once the physics we need to be describing is clear.} We can rewrite equation \ref{rp-cm-hmd-condEquations} as
\begin{equation}\label{rp-hm-HamiltonSymp}
	\begin{aligned}
		S_a = S^b \omega_{ba}  &= \partial_a H 
	\end{aligned}
\end{equation}
which notationally is the same as \ref{rp-cm-hsd-condGeneralizedEquations}. The insight that the displacement field is equal to the gradient of $H$ rotated 90 degrees still applies, except there are now multiple ways, in principle, to do that rotation. It is only the one defined by $\omega_{ab}$ that works.

Conditions \ref{rp-cm-hsd-condDivergenceDisplacement}, \ref{rp-cm-hsd-condUnitaryJacobian}, \ref{rp-cm-hsd-condConservedVolume} and \ref{rp-cm-hsd-condConservedDensity} are still satisfied and equivalent to each other. In fact, the divergence of the displacement field is zero
\begin{equation}
	\partial_a S^a = \partial_{q^i} S^{q^i} + \partial_{p_i} S^{p_i} = \partial_{q^i} \partial_{p_i} H - \partial_{p_i} \partial_{q^i} H = 0
\end{equation}
and the Jacobian is unitary
\begin{equation}
	\begin{aligned}
		\hat{q}^i &= q^i + S^{q^i} \delta t \\ 
		\hat{p}_i &= p_i + S^{p_i} \delta t \\ 
		\partial_{b} \hat{\xi}^a &= \left[\begin{array}{ c c }
			\delta_j^i + \partial_{q^j} S^{q^i} \delta t & \partial_{p_j} S^{q^i} \delta t \\
			\partial_{q^j} S^{p_i} \delta t & \delta_i^j + \partial_{p_j} S^{p_i} \delta t \\
		\end{array} \right] \\
		\left|\partial_{b} \hat{\xi}^a\right| &= \left|\delta_j^i + \partial_{q^j} S^{q^i} \delta t\right| \left|\delta_i^j + \partial_{p_j} S^{p_i} \delta t\right| - \left|\partial_{p_j} S^{q^i} \, \delta t \right| \, \left| \partial_{q^j} S^{p_i} \, \delta t \right| \\
		&= 1 + \left(\partial_{q^i} S^{q^i} + \partial_{p_i} S^{p_i} \right) \delta t + O(\delta t^2)
	\end{aligned}
\end{equation}
since the first-order term is again the divergence. The Jacobian is still the multiplicative factor between past/future areas/densities, and therefore they are conserved even in the case of multiple degrees of freedom.

However, these conditions are not equivalent to \ref{rp-cm-hmd-condEquations}. The displacement field $S^a$ has $2n$ components and is therefore specified by $2n$ functions. The above conditions specify the same single constraint, bringing down to $2n -1$ the number of independent components. The choice of Hamiltonian provides another constraint, leaving $2n - 2$ choices undetermined. In the single degree of freedom case, $n=1$ and no choices would be left, but in the general case this is not enough. Therefore \ref{rp-cm-hmd-condEquations} implies \ref{rp-cm-hsd-condDivergenceDisplacement}, \ref{rp-cm-hsd-condUnitaryJacobian}, \ref{rp-cm-hsd-condConservedVolume} and \ref{rp-cm-hsd-condConservedDensity}, but the converse is not true.

Let's see what happens to condition \ref{rp-cm-hsd-condConservedSymplectic}, the invariance of $\omega$ in the general case. We have
\begin{equation}
	\begin{aligned}
	\hat{\omega}_{ab} &= \partial_a \hat{\xi}^c \omega_{cd} \partial_b \hat{\xi}^d \\ &= \left(\delta_a^c + \partial_a S^c \delta t\right) \omega_{cd} \left(\delta_b^d + \partial_b S^d \delta t\right) \\
	&= \omega_{ab} + \left(\partial_a S^c \omega_{cb} + \omega_{ad} \partial_b S^d \right) \delta t + O(\delta t^2) \\
	&= \omega_{ab} + \left(\partial_a (S^c \omega_{cb}) + \partial_b ( S^d \omega_{ad}) \right) \delta t + O(\delta t^2) \\
	&= \omega_{ab} + \left(\partial_a (S^c \omega_{cb}) - \partial_b ( S^d \omega_{d a}  ) \right) \delta t + O(\delta t^2).
	\end{aligned}
\end{equation}
Therefore, the invariance of $\omega_{ab}$ is equivalent to
\begin{equation}
	\begin{aligned}
	\partial_a &(S^c \omega_{cb} ) - \partial_b (S^c \omega_{ca} ) = 0.
	\end{aligned}
\end{equation}
In terms of the rotated displacement field $S_a$ we have the more compact form
\begin{equation}
	\partial_a S_b - \partial_b S_a = 0.
\end{equation}
This tells us that the rotated displacement field $S_a$ is curl free, which is the same condition as \ref{rp-cm-hsd-condCurlRotatedDisplacement}, therefore \ref{rp-cm-hsd-condCurlRotatedDisplacement} and \ref{rp-cm-hsd-condConservedSymplectic} are equivalent conditions also in the general case.

Note that Hamilton's equations state that the rotated displacement field is the gradient of the Hamiltonian, and therefore
\begin{equation}
	\partial_a S_b - \partial_b S_a = \partial_a (S^c \omega_{cb}) - \partial_b (S^c \omega_{ca}) =  \partial_a \partial_b H - \partial_b \partial_a H = 0,
\end{equation}
which simply verifies that the curl of the gradient is zero. Conversely, if $S_a$ is curl-free, then it admits a scalar potential $H$ such that
\begin{equation}
	S_a = S^b \omega_{ba} = \partial_a H
\end{equation}
which recovers Hamilton's equations. Therefore \ref{rp-cm-hmd-condEquations}, \ref{rp-cm-hsd-condCurlRotatedDisplacement} and \ref{rp-cm-hsd-condConservedSymplectic} are equivalent.

The relationship between Poisson brackets and $\omega^{ab}$ is the same in the general case, therefore \ref{rp-cm-hsd-condConservedPoisson} and \ref{rp-cm-hsd-condConservedSymplectic} are equivalent as well.

To sum up, in the general case \ref{rp-cm-hmd-condEquations}, \ref{rp-cm-hsd-condGeneralizedEquations}, \ref{rp-cm-hsd-condConservedSymplectic}, \ref{rp-cm-hsd-condConservedPoisson} and \ref{rp-cm-hsd-condCurlRotatedDisplacement} are all equivalent and therefore full characterizations of Hamiltonian mechanics in the general case. These imply \ref{rp-cm-hsd-condDivergenceDisplacement}, \ref{rp-cm-hsd-condUnitaryJacobian}, \ref{rp-cm-hsd-condConservedVolume} and \ref{rp-cm-hsd-condConservedDensity}, which are all equivalent to one another, but weaker conditions that cannot recover Hamiltonian mechanics in full. For the second set of conditions, we already have an intuitive geometrical picture: the net flow of the displacement within a region is zero, volumes are preserved and so are densities. We need to build a stronger geometrical intuition for the first set, which is actually the more fundamental one.

Condition \ref{rp-cm-hsd-condConservedSymplectic} tells us that $v^a \omega_{ab} w^b$ is a conserved quantity, no matter what vectors $v^a$ and $w^b$ we choose. In the case of a single degree of freedom, this represented the area of the parallelogram formed by the two vectors, which also was the volume of the region. In the general case, we still have two vectors, but the situation is a bit more complicated.

We can gain an understanding by looking at the outer product decomposition for $\omega_{ab}$ we saw in \ref{rp-cm-hmd-symplecticForm}. This tells us that what happens within a degree of freedom is different from what happens across degrees of freedom. If we pick a single degree of freedom $1 \leq x \leq n$ and two vectors $v = v^q e_{q^x} + v^p e_{p_x}$ and $w = w^q e_{q^x} + w^p e_{p_x}$ that stretch along that degree of freedom, then we have
\begin{equation}
	v^a \omega_{ab} w^b =  v^q w^p - v^p w^q.
\end{equation}
That is, within each degree of freedom, $\omega_{ab}$ computes the area of the parallelogram. Since $\omega_{ab}$ is conserved, parallelograms within any degree of freedom will be mapped to parallelograms of the same size.

If we pick two different d.o.f. $x$ and $y$ and two corresponding vectors $v = v^q e_{q^x} + v^p e_{p_x}$ and $w = w^q e_{q^y} + w^p e_{p_y}$, then we have
\begin{equation}
	v^a \omega_{ab} w^b =  0.
\end{equation}
This defines a notion of orthogonality between different degrees of freedom. Since $\omega_{ab}$ is conserved, this notion of orthogonality is preserved during the evolution: orthogonal degrees of freedom are mapped to orthogonal degrees of freedom.

Those familiar with general relativity and/or Riemannian geometry may gain more insight by the following analogy. In those cases, the metric tensor $g_{ij}$ defines the geometry by defining the scalar product between vectors. That is, given two vectors $v^i$ and $w^j$, $v^i g_{ij} w^j = |v| |w| \cos \theta_{vw}$. Therefore the metric tensor defines the length and angles for vectors. In Cartesian coordinates, the metric tensor is a unitary matrix of the same dimension of the space. The form $\omega_{ab}$ does something in some sense similar and in some sense different. It defines areas within degrees of freedom and angles between them. Hamiltonian evolution preserves these areas and angles.

If areas and orthogonality are preserved, then volumes are preserved as well. The volume of a parallelepiped formed by parallelograms on orthogonal degrees of freedom will simply be the product of the areas of the parallelograms. Therefore we can understand why Hamiltonian mechanics satisfies \ref{rp-cm-hsd-condConservedVolume}. We can also understand why \ref{rp-cm-hsd-condConservedVolume} is not enough to recover Hamiltonian mechanics. An evolution could stretch one degree of freedom while shrinking another by the same amount. The total volume would remain the same, even though the area in each degree of freedom wouldn't. For example, take the system of equations:
\begin{equation}
	\begin{aligned}
	d_t q^1 &= S^{q^1} = \frac{p_1}{m} \\
	d_t p_1 &= S^{p_1} = - b p_1 \\
	d_t q^2 &= S^{q^2} = \frac{p_2}{m} \\
	d_t p_2 &= S^{p_2} = b p_2 \\
	\end{aligned}
\end{equation}
The first degree of freedom is a particle under linear drag, while the second is a particle accelerated (not decelerated) proportionally to its momentum by the same coefficient. We can verify that
\begin{equation}
	\partial_a S^a = \partial_{q^1} \frac{p_1}{m} + \partial_{p_1} (-b p_1) + \partial_{q^2} \frac{p_2}{m} + \partial_{p_2} (b p_2) = - b +b = 0
\end{equation}
the diverges is zero and therefore \ref{rp-cm-hsd-condDivergenceDisplacement} and \ref{rp-cm-hsd-condConservedVolume} are satisfied. However
\begin{equation}
	\partial_{q^1} S_{p_1} - \partial_{p_1} S_{q^1} = \partial_{q^1} S^{q^1} \omega_{q^1 p_1} - \partial_{p_1} S^{p_1} \omega_{p_1 q^1} = \partial_{q^1} \frac{p_1}{m} (1) - \partial_{p_1} (-b p_1) (-1) = -b.
\end{equation}
The curl of $S_a$, then, is not zero, \ref{rp-cm-hsd-condCurlRotatedDisplacement} is not satisfied, neither are \ref{rp-cm-hsd-condConservedSymplectic} and \ref{rp-cm-hmd-condEquations}. The system is not Hamiltonian precisely because we are not preserving the areas within each independent d.o.f.: the first is shrunk and the second stretched.

Now that we have a more precise understanding of the mathematics and the geometry, we should turn to the physics. Note that all the previous physical conditions \ref{rp-cm-hsd-condDetRev}, \ref{rp-cm-hsd-condThermoRev}, \ref{rp-cm-hsd-condInformation} and \ref{rp-cm-hsd-condUncertainty} are equivalent to \ref{rp-cm-hsd-condConservedVolume} and \ref{rp-cm-hsd-condUnitaryJacobian}. Therefore determinism and reversibility is clearly a constitutive assumption of Hamiltonian mechanics in the general case, but it cannot be the only one. Ideally, we would like to find a condition that is independent of \ref{assum_detrev}. However, we saw that \ref{rp-cm-hsd-condConservedSymplectic} implies \ref{rp-cm-hsd-condConservedVolume}, therefore the mathematics does not already give us two independent conditions we can map to the physics.

This is an important aspect to understand for reverse physics: the mapping between physical and mathematical conditions need not necessarily be one to one. A single mathematical condition can map to multiple physical ones, or the same physical condition can map to multiple mathematical ones. We saw before that determinism and reversibility forces the evolution map to be both bijective and volume preserving. Mathematically, these are two independent conditions. We can have a bijection that is not volume preserving (e.g. a linear transformation that stretches one side) or a volume preserving map that is not bijective (e.g. a map from $\mathbb{R}$ to $\mathbb{R}$ that maps all rationals to $0$ while leaving all the irrationals the same). Yet, a physically meaningful deterministic and reversible map must do both. Here we have the opposite: Hamiltonian mechanics implies determinism and reversibility, but is also implying at least another physical condition, and we need to understand which and whether it is physically independent.

Let's start from what we have already established: the phase space volume quantifies the number of states in the region. It stands to reason that the area on each degree of freedom identifies the number of configurations for that degree of freedom. Therefore, given two vectors, $v = v^q e_{q^x} + v^p e_{p_x}$ and $w = w^q e_{q^x} + w^p e_{p_x}$, constrained on a single degree of freedom $x$, the area of the parallelogram they identify, $v^q w^p - v^p w^q$, quantifies the number of configurations. Therefore $\omega_{ab}$ returns the number of configurations within each degree of freedom. What about between degrees for freedom?

As we saw, the conserved volume, which means the total number of states, is the product of those degrees of freedom that are orthogonal. In terms of $\omega_{ab}$, $x$ and $y$ are orthogonal if $\omega_{q^x q^y} = \omega_{q^x p_y} = \omega_{p_x q^y} = \omega_{p_x p_y} = 0$. In this case, the volume is simply the product of the phase space areas on the $x$ and $y$ degrees of freedom. This means that the total number of states is the product of the configurations of each degree of freedom. Physically, it means that a configuration choice for $x$ does not constrain the configurations for $y$. This means that the degrees of freedom are independent.

Given that there are different notions of independence, let us go through an example. Suppose we have a rabbit farm and describe its state with the number of males and females. These two variables are independent: if we say there are 231 females it doesn't, in principle, tell us anything about the number of males. Now, we may expect the population of both sexes to be about equal, and we may even find that in most rabbit farms that is the case, but this does not describe something about the nature of the variables themselves: it describes the nature of rabbit farms. Chicken farms, for example, would be predominantly females, as those are the ones that lay eggs.

Now we could choose to describe the rabbit farm with another set of variables: the number of females and the total number of rabbits. In this case, the variables are not independent. If we find that there are 231 females, it tells, in principle, that there must be at least 231 rabbits. Conversely, if we find that there are 231 rabbits, there can only be up to 231 females. This dependence is not a feature of the rabbit farms. It does not just happen to be that there are no farms where the number of female rabbits exceeds the total number of rabbits. There can't be one.

This type of independence is very different from the notion of independence in terms of statistics and probability. The latter is in terms of whether the probability distribution factorizes. That is, if $P(f,m)$ is the probability that a particular rabbit farm has $f$ females and $m$ males, the distributions of males $P(m)$ and females $P(f)$ are independent if $P(f,m) = P(f) P(m)$. 

We therefore have two notions of independence. One is on the variables themselves and whether they can allow (i.e. whether we can measure) different combinations. One is on the probability distribution we may have in a particular case and whether it factorizes. The orthogonal directions in phase space, then, are independent in the first, stronger, sense. The degrees of freedom themselves are independent, regardless of what probability distribution one may put on top.

One may ask whether there is a link between the two, and in fact there is. Going back to our rabbits, we can easily see that, given any distribution $P(f)$ on the females, we can choose any distribution $P(m)$ on the males and set $P(f,m)=P(f)P(m)$. However, this does not happen for the total number. Suppose we chose $P(f)$ and we wanted to find a $P(f+m)$ such that $P(f, f+m) = P(f)P(f+m)$. The probability of the total number of rabbits could not change based on the number of females. If the probability of having 231 females is non-zero, then the probability of having less than 231 total rabbits in that case must be zero. But since we want the probability of having less than 231 rabbits independent of the number of females, then it must be zero for all cases. That is, the probability $P(f+m)$ must be zero for all numbers smaller than the greatest value of $f$ such that $P(f) \neq 0$. If there is no such greatest value of $f$, for example $f$ follows a geometric distribution, no $P(f+m)$ can exist that is independent of $P(f)$.

[TODO: add a drawing showing that we are essentially trying to find a rectangular area on which to create the joint distribution. The diagonal constraint poses a problem. ]

The conclusion is that only independent variables can support independent distributions.\footnote{Formally, let $(\Omega, \mathcal{F}, P)$ be a probability space, let $X : \Omega \to E_X$ and $Y : \Omega \to E_Y$ be two random variables and $Z : \Omega \to E_X \times E_Y$ be their joint random variable (i.e. $Z(\omega) = (X(\omega), Y(\omega))$. Then $X$ and $Y$ are independent in this stronger sense if $Z(\Omega) = X(\Omega) \times Y(\Omega)$ and are statistically independent if the cumulative distribution function $F_Z(x,y)=F_X(x)F_Y(y)$ factorizes. Alternatively, they are independent if the $\sigma$-algebra generated by the joint distribution $Z$ is the product of the $\sigma$-algebras generated by $X$ and $Y$, and are statistically independent if the $\sigma$-algebras generated by $X$ and $Y$ are independent in the standard probability sense. } It should be clear that this observation is something that goes beyond the physical underpinning of Hamiltonian mechanics: it is something that applies to any variable to which we want to assign a probability distribution. As such, we do not want to expand the scope too much at this point, though clearly we will need to explore this more in full.

Here we limit ourselves to concluding that the following four conditions
\begin{align}
	\tag{HM-IC}\label{rp-cm-hmd-condIndepDof}
	&\text{The system is decomposable into independent d.o.f.} \\
	\tag{HM-ID}\label{rp-cm-hmd-condIndepDistr}
	&\text{The system allows statistically independent distributions over each d.o.f.} \\
	\tag{HM-II}\label{rp-cm-hmd-condIndepEntropy}
	&\text{The system allows informationally independent distributions over each d.o.f.} \\
	\tag{HM-IU}\label{rp-cm-hmd-condIndepUncertainty}
	&\parbox{5in}{The system allows peaked distributions where the uncertainty is the product of the uncertainty on each d.o.f.}
\end{align}
are equivalent. The first means that the count of states factorizes, the second that probability distributions can factorize, the third that the information entropy can sum, and the fourth that the determinant of the covariance matrix can factorize. Since only independent variables can support statistically independent distributions, \ref{rp-cm-hmd-condIndepDof} is equivalent to \ref{rp-cm-hmd-condIndepDistr}. Statistical independence of random variables coincides with independence of information entropy, therefore \ref{rp-cm-hmd-condIndepDistr} is equivalent to \ref{rp-cm-hmd-condIndepEntropy}. The uncertainty for peaked distributions factorizes if and only if the joint distribution is the product of independent distributions, therefore  \ref{rp-cm-hmd-condIndepDistr} is equivalent to \ref{rp-cm-hmd-condIndepUncertainty}.

Clearly these conditions are independent from \ref{assum_detrev}. We can imagine a deterministic and reversible system that cannot be broken into separate independent degrees of freedom, and we can imagine a system that can be broken into separate independent degrees of freedom that does not evolve deterministically or reversibly. The question is whether assuming independent degrees of freedom and deterministic and reversible evolution is enough to recover Hamiltonian mechanics. 

The first thing to check, then, is whether we have enough constraints to recover $\omega_{ab}$. Assuming that the system can be broken up into independent degrees of freedom, we must be able to define the count of configurations for each degree of freedom, and independent degrees of freedom must be orthogonal. The fact that $\omega_{ab}$ will return zero if it acts on directions belonging to independent degrees of freedom is really telling us that $\omega_{ab}$ counts not just configurations, but independent configurations. This, in retrospect, makes sense. But, so far this seems to restrict $\omega_{ab}$ to be
\begin{equation}
	\omega_{ab} = \left[\begin{array}{cc}
		0 & 1 \\
		-1 & 0 
	\end{array} \right] \otimes   \left[ {\begin{array}{cccc}
		a_{1} & 0 & \cdots & 0\\
		0 & a_{2} & \cdots & 0\\
		\vdots & \vdots & \ddots & \vdots\\
		0 & 0 & \cdots & a_{n}\\
\end{array} } \right].
\end{equation}
The fact that $\omega_{ab}$ must return the area in each degree of freedom constrains the left matrix of the outer product to the one above. The fact that $\omega_{ab}$ needs to return zero across independent degrees of freedom constrains the right matrix of the outer product to be diagonal. However, there is nothing, at this point, that seems to constrain the area of each d.o.f. to map to the same count of configurations. Naturally, we could simply rescale conjugate momentum in each d.o.f. to homogenize the count, but this would be an arbitrary freedom. Is there something forcing all the $a_i$ to be the same?

Note that $q^i$ and $p_i$ are not the only variables that form independent degrees of freedom. If we take two independent d.o.f. $x$ and $y$, $x+y$ and $x-y$ will also form independent degrees of freedom. That is, $\omega_{ab}$ defines orthogonality for all d.o.f., not just those of a particular basis. Changing $x$ and $y$ to $x+y$ and $x-y$ will effectively apply a rotation on the diagonal matrix, which will remain diagonal only if the coefficients on the diagonal are the same.

This tells us that if we want $\omega_{ab}$ to properly capture the independence of linear combinations of independent d.o.f., the diagonal matrix must have the same coefficient. This coefficient represents the freedom we have in choosing the units of omega with respect to the units of everything else. We can set the coefficient to 1 if the product between $q^i$ and $p_i$ is in $J\cdot s$ (i.e. the same units of $\hbar$ and angular momentum). Therefore expressing the number of configurations with the same units for all d.o.f. is not an extra constraint, but it is necessary to keep track of the dependency relationship for all degrees of freedom.

Therefore the other constitutive assumption of Hamiltonian mechanics is
\renewcommand{\theassump}{IND}
\begin{assump}[Independent d.o.f.]\label{assum_indep}
	The system is decomposable into independent degrees of freedom.
\end{assump}
\renewcommand{\theassump}{\Roman{assump}}
This assumption leads to conditions \ref{rp-cm-hmd-condIndepDof}, \ref{rp-cm-hmd-condIndepDistr}, \ref{rp-cm-hmd-condIndepEntropy} and
\ref{rp-cm-hmd-condIndepUncertainty}, which we saw implies the existence of a form $\omega_{ab}$ that defines the independence of d.o.f. together with the count of independent configurations for each d.o.f.. Conversely, assuming \ref{rp-cm-hmd-condEquations} means defining an $\omega_{ab}$ such that \ref{assum_indep} is satisfied. The last question is whether \ref{assum_detrev} and \ref{assum_indep} are enough to recover \ref{rp-cm-hmd-condEquations}.

As we saw, \ref{assum_indep} by itself means the existence of the counting form $\omega_{ab}$. Meanwhile, \ref{assum_detrev} by itself means the conservation of the total number of states, the volume. These two mathematical conditions, by themselves, do not lead to Hamiltonian mechanics. We need that the counting form itself is conserved, meaning that d.o.f. independence and the configuration count is preserved. This boils down to the following question: does it make sense, on physical grounds, to have a deterministic and reversible evolution that takes a system decomposable into independent degrees of freedom and turns it into a system that is no longer decomposable? More specifically, can deterministic and reversible evolution take two independent degrees of freedom and break their independence?

We should remind ourselves that independence here is not statistical independence. Clearly, Hamiltonian evolution can add correlations, and therefore evolve the product of two independent distributions into something that is no longer factorizable. This is not the question at hand. The notion of independence on the table is the one that tells us that all the combinations of configurations are at least possible. Recall the example of the rabbits: number of females and total rabbits are not independent because we cannot have more females than rabbits. This is the type of independence we are interested in. Can deterministic and reversible evolution lose this type of independence?

The answer is, as you may expect, no. This is easily seen in the finite case. Suppose we have two integer variables, $1 \leq x \leq 3$ and $1 \leq y \leq 3$. If they are independent, we have a total of $9$ distinct cases. If the evolution is deterministic and reversible, we will still have $9$ distinct cases, which means the variables must remain independent. Note that we may introduce a correlation between $x$ and $y$. But still, we need $9$ total cases.

The issue here is that the case of independent variables is maximal as it posits that all combinations of configurations are possible. Therefore, the only way to make variables no longer independent is to decrease the number of distinct cases, which cannot happen during deterministic and reversible evolution. The same must happen in the infinite or the continuous case, because finite ranges must still be comparable with finite sizes which must hold the same property. Therefore when we take both physical assumptions \ref{assum_indep} and \ref{assum_detrev}, the physics tells us that independence of degrees of freedom must be preserved, which means preserving $\omega_{ab}$ as well. In other words:
\begin{equation}
	\parbox{5in}{Hamiltonian mechanics is exactly the deterministic and reversible evolution of a system decomposable into a finite collection of independent degrees of freedom.}
\end{equation} 

This conclusion is yet another example of why just looking at the math is not enough. Two physical conditions taken separately may each impose one mathematical condition, but it is not necessarily true that imposing them together will only impose the conjunction of the two mathematical conditions. More than a problem with math in general, we believe it is an indication that the math we currently use is not the ``physically correct'' one as it does not seem to be capturing the entirety of the physical conditions. 

Note that, in principle, we could ask the evolution to preserve the independence of d.o.f. without requiring \ref{assum_detrev}. As we saw, fixing all independent d.o.f. fixes $\omega_{ab}$ up to a scalar factor, which could change during the evolution. The volume would stretch or shrink depending on the factor, stretching or shrinking each d.o.f. by the same amount. An example of this would be a particle under linear drag in three dimensions. Since a faster moving object will be subjected to a greater frictional force, a spread in momentum $\Delta p$ will become smaller in time, and will tend to zero as time increases. Given that the friction coefficient is the same for all directions, all degrees of freedom will shrink at the same rate. If we understand the volume as entropy, this tells us that the only way we can add or remove entropy to/from a system while preserving the independence of the degrees of freedom is by dividing that entropy contribution equally among each d.o.f.. In other words, preservation of d.o.f. independence gives us a sort of equipartition of entropy change. It is again striking to find these connections between disciplines at such a basic level.

To conclude, we now have a mathematically and physically precise way to characterize Hamiltonian evolution. We had found that Hamiltonian mechanics did not apply to all cases, and now we know exactly to which cases it applies: systems described by finitely many independent degrees of freedom undergoing deterministic and reversible evolution.

\section{Reversing differential topology}

In the previous sections, we saw that elements of differential topology and differential geometry started appearing: we employed a generalization of the curl and we used the form $\omega_{ab}$ to lower indexes, much like one does in general relativity with the metric tensor $g_{\alpha\beta}$. Since we will use these tools more and more, we should take a detour and understand the physical significance of the tools themselves. We will end up with a generalized notion of integral and of differential operations that work with an arbitrary number of dimensions. We will also conclude that to reach the full potential of reverse physics, we need to apply the same techniques not just to the equations themselves, but to the mathematical tools we use to formulate them.

The main reason we are forced to abandon vector calculus in favor of differential topology and geometry is that vector calculus does not generalize to an arbitrary dimensional space. Special and general relativity, for example, live on a four-dimensional space-time; Hamiltonian and Lagrangian mechanics live in phase-space, which can have an arbitrarily large number of degrees of freedom. Similarly to what we have done with the equations, we will start from the expressions of vector calculus, see their limitations, and construct generalized ones. We warn the reader who is already familiar with differential topology that this process will give us notation and concepts that are slightly different from what is used by mathematicians. We will discuss this difference at the end of the section.

The first tool we need to generalize is that of line, surface and volume integrals. For example, the mass $m$ within a region $V$ can be understood as the sum of the contributions of a mass density $\rho$ within each infinitesimal volume $dV$:
\begin{equation}
	m(V) = \iiint_V \rho dV.
\end{equation}
Similarly, the magnetic flux $\Phi$ through a surface $\Sigma$ can be understood as the sum of the contributions of the magnetic field $\vec{B}$ over each infinitesimal $\vec{d\Sigma}$:
\begin{equation*}
	\Phi(\Sigma) = \iint_\Sigma \vec{B} \cdot \vec{d\Sigma}.
\end{equation*}
Lastly, the work $W$ over a path $\lambda$ can be understood as the sum of the contributions of a force field $\vec{f}$ over each infinitesimal segment $\vec{d\gamma}$:
\begin{equation*}
	W(\gamma) = \int_\gamma \vec{f} \cdot \vec{d\gamma}.
\end{equation*}

Note the pattern: the functionals $W$, $\Phi$ and $m$ all take a region of space while $\vec{f}$, $\vec{B}$ and $\rho$ act on infinitesimal regions. However the pattern is not completely consistent. In the line integral case, we have the product between a vector representing the force and a vector representing the displacement along the line. In the surface integral case we have the product between a pseudo-vector representing the magnetic force and a pseudo-vector representing the normal to the surface element. In the volume integral case we have the product between a pseudo-scalar representing the density and a scalar representing the volume of the infinitesimal region. Each operation is slightly ad-hoc. Moreover, a surface has a single perpendicular direction only in three dimensions. In four dimensions, for example, there are multiple different perpendiculars to the same plane. Lastly, they all require a notion of product between vectors, an inner product, which in differential geometry is only defined on Riemannian spaces, those that define a metric tensor. That is, those spaces in which angles and distances are well defined. In physics we are so used to working with spaces that have an inner product that it may seem that all spaces provide one, but that is not the case. In phase space, there is no way to compare differences in position with differences in momentum, therefore we do not have a metric to take a scalar product; there is no overall notion of distance and angle. In general, if we imagine the space that represents all possible outcomes of a blood test, this will form a manifold as each test can be fully described by a finite set of continuous quantities. In this space, there is no natural notion of distance and angles between directions, there is no notion of geometry. As we will see, the notion of integral, the idea of a quantity that can be understood as the sum of infinitely many infinitesimally small contributions, does not require either a particular number of dimensions or a notion of distance and angle.

Suppose we understood $f$, $B$ and $\rho$ not as above, but as maps that for each infinitesimal region return an infinitesimal contribution $dW$, $d\Phi$ and $dm$. We could simply write
\begin{equation*}
	\begin{aligned}
		W(\gamma) &= \int_\gamma dW = \int_\gamma f(d\gamma) \\
		\Phi(\Sigma) &= \iint_\Sigma d\Phi = \iint_\Sigma B(d\Sigma) \\
		m(V) &= \iiint_V dm = \iiint_V \rho(dV).
	\end{aligned}
\end{equation*}
This pattern is straightforward and more easily generalized. We call $k$-forms these functions of infinitesimal regions, where $k$ is the dimensionality of the infinitesimal region they take as an argument. The force, in this notation, is a one-form (or covector) as it takes one dimensional infinitesimal regions (i.e. vectors); the magnetic field is a two-form; the density is a three-form. We can also say that a scalar field, like the temperature, is a zero-form, as it takes points, zero dimensional objects.

Since $k$-forms act over infinitesimal regions, they will have some key properties.  First, note that each infinitesimal region can be understood as a parallelepiped, and a parallelepiped is fully identified by its sides. Therefore, a $k$-form can be understood as acting on a set of infinitesimal displacements, the sides of the parallelepiped, whose number matches the dimensionality of the form. A one-form will take one displacement, a two-form two displacements and so on. Second, as they are linear functions of the infinitesimal regions, they will also be linear functions of the vectors that define these infinitesimal regions. Lastly, all forms must be anti-symmetric because switching the order of the sides does not change the parallelepiped, but it changes its orientation.

We can write displacements and forms in terms of components and basis elements
\begin{equation}
	\begin{aligned}
	dP &= dx^i e_i \\
		f &= f_i e^i \\
		B &= B_{ij} e^i \otimes e^j \\
		\rho &= \rho_{ijk} e^i \otimes e^j \otimes e^k.
	\end{aligned}
\end{equation}
The anti-symmetry means that switching the indexes introduces a minus since. For example, $B_{ij} = - B_{ji}$ and $\rho_{ijk} = - \rho_{jik}$. Therefore, our integrals can be written as
\begin{equation}
	\begin{aligned}
		W(\lambda) &= \int_\lambda f_i \, dx^i \\
		\Phi(\Sigma) &= \iint_\Sigma B_{ij} \, dx_1^i \, dx_2^j \\
		m(V) &= \iiint_V \rho_{ijk} \, dx_1^i \, dx_2^j \, dx_3^k.
	\end{aligned}
\end{equation}
Each $k$-form, then, is a fully anti-symmetric covariant tensor, with one index for each dimension of the form. In this view, the $B^x$ component of the magnetic field, then, becomes the $B_{yz}$ component. That is, instead of being the component that gives us the magnetic flux along the $x$ direction, it is the component gives us the flux through the $yz$ plane. Similarly, $\rho_{xyz}$ is better understood not as the value of the mass density at a point, but rather the value of the density over the $xyz$ volume. The indexes make unit dependency apparent as well: $B_{yz}$ must be in units of magnetic flux divided by the units of $y$ and $z$; $\rho_{xyz}$ must be in units of mass divided by the units of $x$, $y$ and $z$; in spherical coordinates, $B_{r\theta}$ must be in units of magnetic flux divided by units of $r$ and $\theta$. In fact, one can argue that these tools are here exactly to keep track of the units of the components.

We can go further, and rewrite the integrals in terms of parametrizations of the surfaces and the components of the forms along said parametrization. We have
\begin{equation}
	\begin{aligned}
		W(\lambda) &= \int_\lambda f_i dx^i = \int_{a_u}^{b_u} f_i \partial_u x^i du = \int_{a_u}^{b_u} f_u du \\
		\Phi(\Sigma) &= \iint_\Sigma B_{ij} \, dx_1^i \, dx_2^j = \int_{a_u}^{b_u} \int_{a_v}^{b_v} B_{ij} \, \partial_u x_1^i du \, \partial_v x_2^j dv = \int_{a_u}^{b_u} \int_{a_v}^{b_v} B_{uv} du dv \\
		m(V) &= \iiint_V \rho_{ijk} \, dx_1^i \, dx_2^j \, dx_3^k = \int_{a_u}^{b_u} \int_{a_v}^{b_v} \int_{a_w}^{b_w} \rho_{ijk} \,  \partial_u x_1^i du \, \partial_v x_2^j dv \, \partial_w x_3^k dw \\
		&= \int_{a_u}^{b_u} \int_{a_v}^{b_v} \int_{a_w}^{b_w} \rho_{uvw} du dv dw.
	\end{aligned}
\end{equation}
These expressions are useful to write the integrals in terms of an integral of $k$ variables. This is particularly useful if the surfaces possess different symmetries than the forms, which allows one to use the parametrization appropriately.

We now have expressions that are easier to generalize. If we denote $S^k$ the space of all the $k$-dimensional subregions of an $n$-dimensional manifold, a $k$-functional $F_k : S^k \to \mathbb{R}$ is an additive functional that takes a $k$-surface $\sigma^k$ and returns a number. This can be expressed as
\begin{equation}
	\begin{aligned}
		F_k(\sigma^k) &= \int_{\sigma^k} \omega_k (d\sigma^k) = \int_{\sigma^k} \omega_{i_1 i_2 \cdots i_k} \, dx_1^{i_1} \, dx_2^{i_2} \, \cdots \, dx_k^{i_k} \\
		&= \int_{a_{u_1}}^{b_{u_1}} \int_{a_{u_2}}^{b_{u_2}} \cdots \int_{a_{u_k}}^{b_{u_k}} \omega_{u_1 u_2 \cdots u_k} \, du_1 \, du_2 \, \cdots \, du_k,
	\end{aligned}
\end{equation}
which is the integral of a $k$-form $\omega_k : V^k \to \mathbb{R}$ that takes an infinitesimal $k$-surface $d\sigma^k$, defined by a set of $k$ vectors, and returns a number. We now have a way to express local objects and integrals in a generalized way.

In the spirit of reverse physics, we ask: what are the physical assumptions that are required to use these mathematical objects? Note that we do not measure mass density $\rho$ directly: we measure mass $m$ within a finite region $V$ and the size of the finite region $V$, and we calculate the mass density by dividing the first by the second. The mass $\rho$  is the limit for which the region $V$ shrinks to a point.\footnote{This is essentially the definition of the Radon-Nikodym derivative used in measure theory.} The same happens with the other quantities. We do not measure a force $f$ directly, but rather the work that a force performs, for example, by deforming a spring. In other words, we measure the finite value of the functional $F$ over a finite region $\sigma$, and the form $\omega$ returns the limit of $F$ for infinitesimal regions. Note that it is crucial for $F$ to be additive, that is, if $\sigma_1$ and $\sigma_2$ are two disjoint regions, $F(\sigma_1 \cup \sigma_2) = F(\sigma_1) + F(\sigma_2)$. Physically, it means that the contribution of one sub-region is independent from the other, because if this isn't the case, we cannot assign a unique value to each region. While this may seem a relatively harmless assumption, it may not hold in general. For example, suppose we have a system distributed in space. Given the mass/energy equivalence, the mass is an additive functional only if the interaction between the parts can be neglected. In fact, if the interaction energy at the boundary is so large that it is of the same scale of the mass within the regions, then it is not true that the total mass is simply the mass within each region. Therefore, if we write the mass density $\rho$, we have implicitly assumed that the interaction energy between parts can be neglected. While this is going to be true in a large number of cases, it is something we have to keep in mind. The functional $F$, in fact, is a more general physical object as it may exist even though infinitesimal additivity fails.

If the functional $F$ respects the limits, in the sense that small variations of the region result in small variations of the value of the functional, then we can express the functional $F$ as the integral of a form $\omega$.\footnote{Mathematically, one needs to be precise as to what these small variations are, and what the space of regions is. Effectively, we need to define what it means for $k$-surfaces to be differentiable. This is more in the scope of physical mathematics then reverse physics.} Note that the standard mathematical definitions are inverted with respect to what makes physical sense. Mathematically, we first define the form and then its integration, and we may worry whether the integral exists or diverges; physically, we first define the functional of finite regions and worry about the existence of an infinitesimal limit, the form, under additional physical assumptions. This is important because, as we said before, it tells us that it is the functional that survives when the assumption fails, not the form. It also means that, if we want to get a robust physical intuition, we should concentrate on the functionals, the finite objects, rather than the forms, the infinitesimal objects.

We would be tempted to conclude something along the lines of
\begin{equation}
	\parbox{5in}{Differential $k$-forms represent the infinitesimal contributions of an infinitesimally additive quantity $F_k$ that depends on a $k$-dimensional surface.}
\end{equation} 
The issue here is that while we have a sense that, mutatis mutandi, an infinitesimally additive functional corresponds to a differential form, we would need to show that differential forms cannot represent anything else. We currently do not have such an argument, though we suspect it should exist. One may first note that no measurement can really be conducted at a point, and therefore it has to extend, at least in principle, over a $k$-dimensional region. Therefore we could argue that all measurements are functionals, additive or not. This means that we would need a map between the value of the components of the form at all points and the value of the functional over all regions. To reach our goal, we would need to show that this map can be established only if the functional is infinitesimally additive, which may not be a strong enough condition by itself. While we are already implicitly assuming the map to be differentiable, we could impose further requirements on the measurement functional. We could impose locality, in the sense that changes of the form within a region must change the value of the functional for said region; it would also mean that changes outside of the region do not affect measurements in the region. While we do not know whether these requirements are sufficient, these are the types of questions we would need to answer in order to have a fully physically meaningful treatment of differential forms.

Having generalized the idea of integration, let us now turn to differential operators such as gradient, curl and divergence. Suppose that $T$ is a scalar field, like the temperature. Then we have the following relationship
\begin{equation}\label{rp-cm-dg-gradientTheorem}
	\int_{\lambda} \vec{\nabla} T \cdot d\vec{\lambda} = T(B) - T(A).
\end{equation}
That is, the integral of the gradient of $T$ along $\lambda$ gives us the difference of $T$ evaluated at the endpoints, the boundary of the line. If $\vec{f}$ is a vector field, like the force, we have
\begin{equation}\label{rp-cm-dg-curlTheorem}
	\int_{\Sigma} \vec{\nabla} \times \vec{f} \cdot d\vec{\Sigma} = \int_{\lambda = \partial \Sigma} \vec{f} \cdot d\vec{\lambda} = W(\lambda)
\end{equation}
That is, the integral of the curl of $\vec{f}$ over a surface $\Sigma$ is equal to the line integral of $\vec{f}$ over the boundary. If $\vec{B}$ is a pseudo-vector field, like the magnetic field, we have
\begin{equation}\label{rp-cm-dg-divergenceTheorem}
	\int_{V} \vec{\nabla} \cdot \vec{B} \, dV = \int_{\Sigma = \partial V} \vec{B} \cdot d\vec{\Sigma} = \Phi(\Sigma).
\end{equation}
That is, the integral of the divergence of $\vec{B}$ over a region $V$ is equal to the surface integral of $\vec{B}$ over the boundary. Note the pattern: the integral of the differential operator applied to the bulk becomes the integral of the original object over the boundary.

Let us understand how this works in terms of functionals. Given the temperature $T(P)$, we can construct a line functional that for each line returns the difference in temperature at the endpoints. Given the work line functional $W(\lambda)$, we can construct a surface functional that for each surface returns the work needed to go around the contour. Given the magnetic flux surface functional $\Phi(\Sigma)$, we can construct a volume functional that for each volume returns the magnetic flux over the boundary. In general, suppose we have a $k$-functional $F_k : S^k \to \mathbb{R}$. To each $k$-surface $\sigma^k$ we can associate a quantity $F_k(\sigma^k)$. Now, suppose we are given a $(k+1)$-surface $\sigma^{k+1}$. While $F_k$ cannot act on $\sigma^{k+1}$, the boundary $\partial \sigma^{k+1}$ is a $k$-surface, therefore we can evaluate $F_k(\partial \sigma^{k+1})$. Therefore, we can define the $(k+1)$-functional $\partial F_k$ such that $\partial F_k (\sigma^{k+1}) \mapsto F_k(\partial \sigma^{k+1})$, which we call the exterior functional of $F_k$.\footnote{Mathematically, we would have to prove that $\partial F_k$ is a functional. Again, these mathematical details are left for the physical mathematics section. For now, we are more interested in the conceptual understanding.} Since the exterior functional is an additive functional, it will have a corresponding form that acts on the infinitesimal region. Conceptually, the gradient of $T$ is the one-form that corresponds to the boundary functional of $T$; the curl of $\vec{f}$ is the two-form that corresponds to the boundary functional of the work $W$, the line integral of $\vec{f}$; the divergence of $\vec{B}$ is the three-form that corresponds to the boundary functional of the magnetic flux $\Phi$, the surface integral of $\vec{B}$. The problem is, again, that the gradient, curl and divergence are not expressed in a way that is easy to generalize.

Given that all three operators are in terms of $\nabla$, which in components is written $\partial_i$, we would like to write something along the lines of 
\begin{equation}\label{rp-cm-dg-generalizedStokes}
	\begin{aligned}
		\partial F_k(\sigma^{k+1}) &=\int_{\sigma^{k+1}} \partial_{i_0} \wedge \omega_{i_1 i_2 \cdots i_k} dx^{i_0} dx^{i_1} dx^{i_2} \cdots dx^{i_k} \\
		&= \int_{\partial \sigma^{k+1}} \omega_{i_1 i_2 \cdots i_k} dx^{i_1} dx^{i_2} \cdots dx^{i_k} = F_k(\partial \sigma^{k+1}).
	\end{aligned}
\end{equation}
The operation $\wedge$, which we call exterior product, must be such that we recover something consistent to the previous operations. That is, we need
\begin{equation}
	\begin{aligned}
		\partial_i \wedge T &= \partial_i T \\
		\partial_i \wedge F_j &= \partial_i F_j - \partial_j F_i \\
		\partial_i \wedge B_{jk} &= \partial_{i} B_{jk} + \partial_{j} B_{ki} + \partial_{k} B_{ij} 
	\end{aligned}
\end{equation}
These would be the expressions we need to recover the gradient, curl and divergence respectively. Let us study them to see the pattern. Fist of all, each expression takes a $k$-form and returns a $(k+1)$-form by adding a derivation along each index. Given that we have a derivation for each index, the number of terms matches the number of indexes of the final form, which is $k+1$. Each terms changes index by taking a cyclic permutation. Recall that the forms are anti-symmetric, therefore each permutation of two indexes introduces a minus sing. A cyclic permutation of $k+1$ elements corresponds to $k$ pair swaps. If $k+1$ is odd, then, each cyclic permutations will correspond to an even number of sign switches, which cancel out. The pattern, then, generalizes in the following way
\begin{equation}
	\begin{aligned}
		\partial_{i_0} \wedge \omega_{i_1 i_2 \cdots i_k} &= \partial_{i_0} \omega_{i_1 i_2 \cdots i_k} + (-1)^{k} \partial_{i_1} \omega_{i_2 \cdots i_k i_0} + (-1)^{2\cdot k} \partial_{i_2} \omega_{i_3 \cdots i_k i_0 i_1} + \cdots \\
		&+ (-1)^{k\cdot k} \partial_{i_k} \omega_{i_0 i_1 \cdots i_{k-1}} \\
		&= \sum_{j=0}^{j=k} (-1)^{j\cdot k} \partial_{i_{j \bmod k+1}} \omega_{i_{j+1 \bmod k+1} i_{j+2 \bmod k+1} \cdots i_{j+k \bmod k+1}}.
	\end{aligned}
\end{equation}
This gives us a fully anti-symmetric tensor which matches the gradient, curl and divergence in the simple cases.\footnote{In principle, we could write the expression in different equivalent ways. Here we used cyclic permutations as it gives a more elegant expression.} We call this operation exterior derivative.

While we have the expression for the exterior derivative, we would like to understand why and how does the expression work. Geometrically, we can imagine integrating along a parallelepiped, which becomes
\begingroup
\allowdisplaybreaks
\begin{align*}
		\int_{\sigma^{k+1}} \partial \wedge \omega_k (\sigma^{k+1}) &= \int_{\sigma^{k+1}} \partial_{i_0} \wedge \omega_{i_1 i_2 \cdots i_k} \, dx_0^{i_0} \, dx_1^{i_1} \, \cdots \, dx_k^{i_k} \\
		&= \int_{a_{u_0}}^{b_{u_0}} \int_{a_{u_1}}^{b_{u_1}} \cdots \int_{a_{u_k}}^{b_{u_k}} (\partial_{u_0} \omega_{u_1 u_2 \cdots u_k} + (-1)^{k} \partial_{u_1} \omega_{u_2 \cdots u_k u_0} \\
		&+ \cdots 
		+ (-1)^{k^2} \partial_{u_k} \omega_{u_0 u_1 \cdots u_{k-1}} )du_0 \,du_1 \, du_2 \, \cdots \, du_k \\
		&= \int_{a_{u_1}}^{b_{u_1}} \cdots \int_{a_{u_k}}^{b_{u_k}} \int_{a_{u_0}}^{b_{u_0}} du_0 \partial_{u_0} \omega_{u_1 u_2 \cdots u_k} \,du_1 \, du_2 \, \cdots \, du_k \\
		&+ (-1)^{k} \int_{a_{u_0}}^{b_{u_0}} \int_{a_{u_2}}^{b_{u_2}} \cdots \int_{a_{u_k}}^{b_{u_k}} \int_{a_{u_1}}^{b_{u_1}} du_1 \partial_{u_1}  \omega_{u_1 u_2 \cdots u_k} \,du_0 \, du_2 \, \cdots \, du_k \\
		&+ \cdots + (-1)^{k^2} \int_{a_{u_0}}^{b_{u_0}} \int_{a_{u_1}}^{b_{u_1}} \cdots \int_{a_{u_{k}}}^{b_{u_k}} du_k  \partial_{u_k} \omega_{u_0 u_1 \cdots u_{k-1}} \,du_0 \, du_1 \, \cdots \, du_{k-1} \\
		&= \left[ \int_{a_{u_1}}^{b_{u_1}} \cdots \int_{a_{u_k}}^{b_{u_k}} \omega_{u_1 u_2 \cdots u_k} \,du_1 \, du_2 \, \cdots \, du_k \right]_{a_{u_0}}^{b_{u_0}} \\
		&+ (-1)^{k} \left[ \int_{a_{u_0}}^{b_{u_0}} \int_{a_{u_2}}^{b_{u_2}} \cdots \int_{a_{u_k}}^{b_{u_k}}\omega_{u_2 \cdots u_k u_0} \,du_0 \, du_2 \, \cdots \, du_k \right]_{a_{u_1}}^{b_{u_1}} \\
		&+ \cdots + (-1)^{k^2} \left[ \int_{a_{u_0}}^{b_{u_0}} \int_{a_{u_1}}^{b_{u_1}} \cdots \int_{a_{u_{k-1}}}^{b_{u_{k-1}}} \omega_{u_0 u_1 \cdots u_{k-1}} \,du_0 \, du_1 \, \cdots \, du_{k-1} \right]_{a_{u_{k}}}^{b_{u_k}} \\
		&= \int_{\partial \sigma^{k+1}} \omega_k (d\partial\sigma^{k+1}).
\end{align*}
\endgroup
What happens is that each direction of integration will match a derivative in the same direction, and therefore will reduce to the integration of $\omega$ on opposing sides of the parallelepiped. This happens for each direction, and therefore the whole integral will reduce to the integration of $\omega$ on the surface of the parallelepiped. We have verified that equation \ref{rp-cm-dg-generalizedStokes}, which is known as the generalized Stokes' theorem, indeed works and it includes, as particular cases, the gradient theorem \ref{rp-cm-dg-gradientTheorem}, the curl theorem \ref{rp-cm-dg-curlTheorem} and the divergence theorem \ref{rp-cm-dg-divergenceTheorem}.

Another elements of vector calculus are the following identities
\begin{equation}
	\begin{aligned}
		&\vec{\nabla} \times \vec{\nabla} T = 0 \\
		&\vec{\nabla} \cdot \vec{\nabla} \times \vec{f} = 0.
	\end{aligned}
\end{equation}
To generalize them, we note that the exterior product $\wedge$ is an anti-commutative and associative operation. Therefore we have
\begin{equation}
	\begin{aligned}
		\partial_{i} \wedge \partial_{j} \wedge \omega_{l_1 l_2 \cdots l_{k}} &= (\partial_i \partial_j - \partial_j \partial_i) \wedge \omega_{l_1 l_2 \cdots l_{k}} = 0 \wedge \omega_{l_1 l_2 \cdots l_{k}} = 0.
	\end{aligned}
\end{equation}
In other words, the exterior derivative applied twice returns zero, no matter on what form it is applied. Given that the curl is the exterior derivative applied to one forms and the gradient is the exterior derivative applied to zero form, the fact that the curl of the gradient is zero is simply an application of the more general property. The same applies for the divergence of the curl. As we saw, the mathematically the property is easy enough to verify, but we get no insight in its meaning. To understand the geometrical significance of the generalized relationship, recall that the exterior derivative of the form is associated with the exterior functional. We should, then, look at what happens when we construct the exterior functional of an exterior functional. We have
\begin{equation}
	\begin{aligned}
		\partial \partial F_k (\sigma^{k+2}) &= \partial F_k (\partial \sigma^{k+2}) = F_k (\partial \partial \sigma^{k+2}) = F_k(\emptyset) = 0.
	\end{aligned}
\end{equation}
In words, the exterior of the exterior functional of $F_k$ equals $F_k$ applied to the boundary of the boundary. However, the boundary of a boundary is always the empty set, and any functional applied to the empty set must be zero. In fact, since functionals are additive, we must have
\begin{equation}
	\begin{aligned}
		F_k(\sigma_k) &= F_k(\sigma_k \cup \emptyset) = F_k(\sigma_k) + F_k(\emptyset).
	\end{aligned}
\end{equation}
Therefore, we can see that the identity $\partial \wedge \partial \wedge \omega = 0$ for every form $\omega$ is ultimately a direct consequence that $\partial \partial U = \emptyset$ for every set $U$.\footnote{Note that we are talking about the boundary of a manifold, not the boundary of a set in the topological sense.} This shows how studying differential relationship in terms of finite functionals can give a more geometrically meaningful picture.

The last element of vector calculus we need to generalize is the idea of potential. That is,
\begin{equation}
	\begin{aligned}
		\vec{\nabla} \times \vec{f} = 0 &\Rightarrow \vec{f} = \nabla V \\
		\vec{\nabla} \cdot \vec{B} = 0 &\Rightarrow \vec{B} = \nabla \times \vec{A} .
	\end{aligned}
\end{equation}
These are generalized by the following formula
\begin{equation}
	\begin{aligned}
		\partial_i \wedge \omega_{l_1 l_2 \cdots l_{k}} = 0 &\Rightarrow   \omega_{l_1 l_2 \cdots l_{k}} = \partial_{l_1} \wedge \theta_{l_2 \cdots l_{k}} .
	\end{aligned}
\end{equation}
That is, if the exterior derivative of a $k$-form is zero, then there exists a $(k-1)$-form whose exterior derivative is the original $k$-form.\footnote{
Technically, closed forms (i.e. those whose exterior derivative is zero) are not necessarily exact forms (i.e. those that are the exterior derivative of another form). This is true only on contractible regions (i.e. those regions that can be continuously shrunk to a point, that do not have holes). While this is a subtle mathematical point, we can understand it by looking at the corresponding functionals. An exact form corresponds to a functional that returns zero for any closed surface. A closed form, however, is guaranteed to return zero only on closed surfaces that are contractible, that can be continuously shrunk to a point. For example, the functional associated with a closed form may return non-zero over a closed surface that encloses a hole, something the functional associated with an exact form cannot do. Therefore, all exact forms are closed, but not all closed forms are exact. However, if we restrict ourselves to a contractible region, the two definitions are the same.}

To sum up, we have generalized the idea of integration over $k$-dimensional submanifolds of an $n$-dimensional space, which leads to the idea of $k$-functionals over finite regions and $k$-forms over the infinitesimal ones; we have seen that the finite functionals are physically more fundamental than the infinitesimal forms. We have seen how $k$-functionals induce $(k+1)$-functionals by acting on the boundaries of the $k+1$ dimensional regions. We have seen that the exterior derivative gives us the form associated to the exterior functionals, and that this operations generalizes the notion of gradient, curl and divergence. While this does not exhaust all that can be done in differential topology and differential geometry, this is enough for what we need to use in the following sections.

Those already familiar with differential topology will have noticed that our notation and definitions do not quite match the ones typically use in math textbooks. The issue is that said notation and definitions do not match what we need to physically capture, and therefore it would be a mistake to employ them. Let us briefly see why. First of all, vectors are typically defined as directional derivative. That is, a vector $v = v^i \partial_i$ is an operator that acts on scalar functions. A velocity, which we typically think of as a vector, is not a derivation. A covector $\theta = \theta_i dx^i $ is a map from a vector to a scalar number. Conjugate momentum, which we typically think of as a covector, is not a map. A differential $dx^i$ is a covector, and it is the exterior derivative of the coordinate $x^i$. This means that differentials are maps such that $dx^i(\partial_{x^j}) = \delta^i_j$. In physics, this is not how we think of differentials and integration. In fact, consider the expression $\int f_i dx^i$. To write it in terms of invariant objects, we would have $f = f_i e^i$ and $dX = dx^i e_i$, where $e^i$ and $e_i$ are the basis and co-basis respectively, so that we obtain $\int f( dX ) = \int f^i e_i(dx^i e_i) = \int f_i dx^i$. Therefore the differential $dX$ is a vector, where the $dx^i$ are the contravariant components, while $f$ is a map from a vector $dX$ to the differential $df$, which is what is integrated. Therefore the differential is the vector, while the force is the covector. The map does not work conceptually. Moreover, in differential topology there is a notion of a single tangent space where all vectors live, which is not compatible with the idea of units. Consider the basis $\partial_i$. Given that coordinates are expressed in different units, we cannot simply sum derivatives along different directions. For example, in polar coordinates $\partial_r$ may have units of inverse meters while $\partial_\theta$ of inverse radians. Worse of all, a directional derivative is taken with respect to a parameter, which also will have units and physical dimension. For example, if $v$ represents a velocity, the components $v^i$ would also depends on the units of space and time. This would mean that units of vectors, the tangent space of a manifold, depend not only on the physical dimensions of the space, but also on the physical dimensions of all possible parameters along which we may want to define a derivation. This would means that the tangent space is not definable only in terms of the units of the manifold itself, and therefore is not defined just in terms of the manifold itself.

The takeaway message here is the following: the mathematical tools we inherit from mathematics are not necessarily designed to capture the physical relationships we need to capture. Mathematicians only care about formal definitions, regardless of what, or if, they represent physically. In physics we do not have this luxury. If we want to have meaningful physical theories, which is ultimately the goal of reverse physics, we need to revisit the mathematical tools we use to formulate them.


\section{Reversing Lagrangian mechanics}

Now that we have a good geometrical and physical feel for Hamiltonian mechanics, and that we have a general understanding of what the tools of differential topology describe physically, we will analyze Lagrangian mechanics more in detail. Conceptually, we already know that Lagrangian mechanics is Hamiltonian mechanics plus assumption \ref{assum_kineq}. We will see that the displacement field admits a vector potential and the Lagrangian is the scalar product between that potential and the displacement field itself. The principle of least action, instead, is geometrically equivalent to asking for paths that are always tangent to the displacement field. Moreover, the principle of least action is really a property of Hamiltonian evolution, and it does not strictly require \ref{assum_kineq}.

\subsubsection{On the physical meaning of the Lagrangian}

The first question we want to have a clear answer for is: what does the Lagrangian represent physically? It is typically introduced as the difference between kinetic and potential energy. However, this does not work in general. Take the Lagrangian for a particle with charge $\mathfrak{q}$ under an electromagnetic field with electric potential $V$ and magnetic potential $A_i$.
\begin{equation}
	L = \frac{1}{2} m |v^i|^2 + \mathfrak{q} v^i A_i -\mathfrak{q} V.
\end{equation}
The first term is clearly potential energy, the last clearly potential energy, but what about the middle term? It depends on velocity, so it would appear to be a kinetic term, but it also depends on the potential. It's both and neither. What about the energy of an electromagnetic field: should we consider that kinetic or potential? The characterization of Lagrangian as difference between kinetic and potential energy, then, works for some systems but not in general.

Another problem in understanding what the Lagrangian represents is that it is not unique. Now, a similar problem exists for the Hamiltonian, in the sense that we can sum an arbitrary constant to any Hamiltonian without changing the equations of motions. Physically, this tells us that there is an arbitrariness of the zero of energy. However, the degeneracy for a Lagrangian is far worse. For example, let $f(x^i,t)$ be an arbitrary function of position and time. We can set
\begin{equation}
	L' = L + \partial_{x^i} f(x^i, t) v^i + \partial_t f(x^i, t).
\end{equation}
We have 
\begin{equation}
	\begin{aligned}
		\partial_{x^i}L' - d_t \partial_{v^i} L' &= \partial_{x^i} L + \partial_{x^i} \partial_{x^j} f v^j + \partial_{x^i} \partial_t f - d_t \left( \partial_{v^i} L + \partial_{x^i} f \right) \\
		&= \partial_{x^i} L - d_t \partial_{v^i} L + \partial_{x^i} \partial_{x^j} f v^j + \partial_{x^i} \partial_t f - \partial_{x^j}\partial_{x^i} f d_t x^j - \partial_t \partial_{x^i} f d_t t \\
		&= \partial_{x^i} L - d_t \partial_{v^i} L + \partial_{x^i} \partial_{x^j} f v^j + \partial_{x^i} \partial_t f - \partial_{x^j}\partial_{x^i} f v^j - \partial_t \partial_{x^i} f \\
		&= \partial_{x^i} L - d_t \partial_{v^i} L.
	\end{aligned}
\end{equation}
That is, the equation of motions given by $L'$ are the same as the ones given by $L$. Therefore the actual value, or the difference of values, of the Lagrangian is physically meaningless. This makes the question even more puzzling: what is the Lagrangian?

\subsubsection{The extended phase space}

Given that we have a good understanding of Hamiltonian mechanics, let's work on the equations that link the two. We have
\begin{equation}
	\begin{aligned}
		L &= p_i v^i - H \\
		&= p_i d_t q^i - H d_t t \\
		&= 
		\begin{bmatrix}
			p_i & 0 & -H
		\end{bmatrix} 
	\raisebox{-1.2em}{$
		\begin{bmatrix}
			d_t q^i \\
			d_t p_i \\
			d_t t
		\end{bmatrix}
		$}.
	\end{aligned}
\end{equation}
The Lagrangian, then, can be understood as the scalar product of two vectors. The second one looks like the displacement vector, however it is the displacement vector not just in phase space, but in phase space extended by time. Let us, then, redefine $\xi^a = [q^i, p_i , t]$ to include the time variable, and have
\begin{equation}
	\begin{aligned}
		S^a &= d_t \xi^a = \begin{bmatrix}
			d_t q^i & d_t p_i & d_t t
		\end{bmatrix} \\
		\theta_a &= \begin{bmatrix}
			p_i & 0 & - H
		\end{bmatrix}.
	\end{aligned}
\end{equation}

We also need to generalize $\omega_{ab}$ to the extended phase space. Looking at equation \ref{rp-cm-hsd-condGeneralizedEquations}, the idea is to put the gradient of the Hamiltonian in the time component. If we set
\begin{equation}\label{rp-cm-lm-contactForm}
	\tag{CF}
	\omega_{ab} = \left[\begin{array}{ccc}
		\omega_{q^i q^j} & \omega_{q^i p_j} & \omega_{q^i t} \\
		\omega_{p_i q^j} & \omega_{p_i p_j} & \omega_{p_i t} \\
		\omega_{t q^j} & \omega_{t p_j} & \omega_{t t} 
	\end{array} \right]= \left[\begin{array}{ccc}
		0 & I_n & \partial_{q^i} H \\
		- I_n & 0 & \partial_{p_i} H \\
		- \partial_{q^j} H & -\partial_{p_j} H & 0
	\end{array} \right] ,
\end{equation}
we have
\begin{equation}
	\begin{aligned}
		S^a \omega_{a q^j} &= S^{q^i}\omega_{q^i q^j} + S^{p_i}\omega_{p_i q^j} + S^{t} \omega_{t q^j} \\
		&= - S^{p_j} - S^{t} \partial_{q^j} H = - S^{p_j} -  \partial_{q^j} H = 0 \\
		S^a \omega_{a p_j} &= S^{q^i}\omega_{q^i p_j} + S^{p_i}\omega_{p_i p_j} + S^{t} \omega_{t p_j} \\
		&= S^{q^j} - S^{t} \partial_{p_j} H = S^{q^j} -  \partial_{p_j} H = 0 \\
		S^a \omega_{a t} &= S^{q^i}\omega_{q^i t} + S^{p_i}\omega_{p_i t} + S^{t} \omega_{t t} \\
		&= S^{q^i} \partial_{q^i} H + S^{p_i} \partial_{p_i} H \\
		&= \partial_{p_i} H \partial_{q^i} H - \partial_{q^i} H \partial_{p_i} H = 0.
	\end{aligned}
\end{equation}
This means that, on the phase space extended by time, Hamilton's equations become
\begin{equation}\label{rp-cm-lm-ExtPSEquation}
	\tag{HM-1E}
	S_a = S^b \omega_{ba} = 0.
\end{equation}
Note that while the position and momentum components of $S_a$ still perform a rotation, the time component does not. So we can't understand $S_a$ as a rotated displacement. Recall that $v^a \omega_{ab} w^b = v_b w^b$ quantified the number of states in the parallelepiped formed by $v^a$ and $w^b$. If $v_b w^b = 0$, then the parallelepiped does not identify states on an independent d.o.f.. For each vector $v^a$, the covector $v_a$ identifies the direction that forms an independent d.o.f. with $v^a$. If we only have position and momentum, we can see that you get the direction rotated by ninety degrees along each d.o.f.. If we extend phase space with time, time does not add a new independent degree of freedom. In fact, the direction given by the displacement field $S^a$ should give us no new independent states: there should be no direction $v^b$ in phase space such that $S^a \omega_{ab} v^b \neq 0$. In other words, $S_b = S^a \omega_{ab}$ must be zero and this is what equation \ref{rp-cm-lm-ExtPSEquation} says.

\subsubsection{Potential of the flow - 1 d.o.f. case}

We still need to understand what $\theta_a$ is. If we compare it to  $\omega_{ab}$, we note that the first has the Hamiltonian as component, while the second has its derivative. Given that $\omega_{ab}$ is anti-symmetric, it is a two-form, we may want to calculate the anti-symmetrized derivative of $\theta_a$, the exterior derivative. We have
\begin{equation}
	\begin{aligned}
		\partial_a \theta_b - \partial_b \theta_a &=
		\left[\begin{array}{ccc}
			\partial_{q^i} \theta_{q^j} - \partial_{q^j} \theta_{q^i} & \partial_{q^i} \theta_{p_j} - \partial_{p_j} \theta_{q^i} & \partial_{q^i} \theta_{t} - \partial_{t} \theta_{q^i} \\
			\partial_{p_i} \theta_{q^j} - \partial_{q^j} \theta_{p_i} & \partial_{p_i} \theta_{p_j} - \partial_{p_j} \theta_{p_i} & \partial_{p_i} \theta_{t} - \partial_{t} \theta_{p_i} \\
			\partial_{t} \theta_{q^j} - \partial_{q^j} \theta_{t} &
			\partial_{t} \theta_{p_j} - \partial_{p_j} \theta_{t} &
			\partial_{t} \theta_{t} - \partial_{t} \theta_{t} 
		\end{array} \right] \\
		&= \left[\begin{array}{ccc}
			\partial_{q^i} p_j - \partial_{q^j} p_i & \partial_{q^i} 0 - \partial_{p_j} p_i & \partial_{q^i} (-H) - \partial_{t} p_i \\
			\partial_{p_i} p_j - \partial_{q^j} 0 & \partial_{p_i} 0 - \partial_{p_j} 0 & \partial_{p_i} (-H) - \partial_{t} 0 \\
			\partial_{t} p_j - \partial_{q^j} (-H) &
			\partial_{t} 0 - \partial_{p_j} (-H) &
			\partial_{t} (-H) - \partial_{t} (-H) 
		\end{array} \right] \\
		&= \left[\begin{array}{ccc}
		0 - 0 & 0 - \delta_i^j & - \partial_{q^i} H - 0 \\
		\delta^i_j - 0 &  0 - 0 & - \partial_{p_i} H - 0 \\
		0 + \partial_{q^j} H &
		0 + \partial_{p_j} H &
		- \partial_{t} H + \partial_{t} H 
	\end{array} \right] \\
		&= \left[\begin{array}{ccc}
			0 & -\delta_i^j & - \partial_{q^i} H \\
			\delta^i_j &  0 & - \partial_{p_i} H \\
			\partial_{q^j} H &
			\partial_{p_j} H &
			0
		\end{array} \right].
	\end{aligned}
\end{equation}
This means that
\begin{equation}\label{rp-cm-lm-formPotential}
	\begin{aligned}
	\omega_{ab} &= - (\partial_a \theta_b - \partial_b \theta_a) = - \partial_a \wedge \theta_b \\
\theta_a &= \begin{bmatrix}
	p_i & 0 & - H
\end{bmatrix},
	\end{aligned}
\end{equation}
the form $\omega_{ab}$ is minus the exterior derivative of $- \theta_a$. In other words, $\omega_{ab}$ has a null exterior derivative and $\theta_a$ is its potential.

Given that the extended phase space for a single degree of freedom is three dimensional, we can use standard vector calculus for a simple physical understanding. In this case, we have
\begin{equation}
	\begin{aligned}
	\theta_a &= \begin{bmatrix}
		p & 0 & - H
	\end{bmatrix} \\
\omega_{ab} &= \left[\begin{array}{ccc}
	0 & 1 & \partial_{q} H \\
	- 1 & 0 & \partial_{p} H \\
	- \partial_{q} H & -\partial_{p} H & 0
\end{array} \right]
= \left[\begin{array}{ccc}
	0 & S^t & - S^p \\
	- S^t & 0 & S^q \\
	S^p & - S^q & 0
\end{array} \right] = \epsilon_{abc}S^c
	\end{aligned}
\end{equation}
where $\epsilon_{abc}$ is the fully anti-symmetric Levi-Civita symbol which returns the sign of the permutation of the variables (i.e. $\epsilon_{qpt} = \epsilon_{ptq} = \epsilon_{tqp} = 1$ while $\epsilon_{tpq} = \epsilon_{pqt} = \epsilon_{qtp} = -1$). We then have the following relationship
\begin{equation}\label{rp-cm-lm-displacementFormComponents}
	\begin{aligned}
	\epsilon_{abc}S^c &= \omega_{ab} = - \partial_a \wedge \theta_b = - (\partial_a \theta_b - \partial_b \theta_a) \\
	S^c &= - \partial_a \times \theta_b \\
	\end{aligned}
\end{equation}
This is the same relationship we would have between the magnetic field $B^i$ and its vector potential $A_i$. Therefore the same vector calculus apply: $S^a$ is divergenceless, admits a vector potential and the flow of the displacement over a closed surface is zero. We can understand this as assumption \ref{assum_detrev} implemented in the extended phase space. What is peculiar, however, is that the vector potential $\theta_a$ is fully characterized by a single arbitrary component, the time component. The vector potential $A_i$, instead, is characterized by two arbitrary components. Why is that?

Suppose that we know that $S^a$ is a divergenceless field, we say that it is minus the curl of a potential
\begin{equation}
	\theta_a = \begin{bmatrix}
	\theta_q & \theta_p & \theta_t
\end{bmatrix} .
\end{equation}
Vector potentials are defined up to the gradient of an arbitrary function, up to a gauge, since $\nabla \times (\theta + \nabla f) = \nabla \times \theta$. We can choose $f$ such that $\partial_p f = - \theta_p$, and therefore we can set, without loss of generality, the momentum component to zero
\begin{equation}
	\theta_a = \begin{bmatrix}
		\theta_q & 0 & \theta_t
	\end{bmatrix} .
\end{equation}
So far, this procedure can be applied to any potential of a divergenceless field. However, the displacement field $S^a$ is particular in that its time component $S^t = d_t t = 1$ is unitary. That is, states flow at the same rate in time. Therefore we must have
\begin{equation}
	S^t = - (\partial_q \theta_p - \partial_p \theta_q) = \partial_p \theta_q = 1.
\end{equation}
Integrating the relationship we find that $\theta_q = p + c$, where $c$ is an arbitrary constant that we can set to zero without loss of generality. We have
\begin{equation}
	\theta_a = \begin{bmatrix}
		p & 0 & \theta_t
	\end{bmatrix}.
\end{equation}
At this point, we simply rename the last component to $-H$ and have
\begin{equation}
	\theta_a = \begin{bmatrix}
		p & 0 & -H
	\end{bmatrix}.
\end{equation}
The form of the vector potential, then, is set by the fact that the flow of $S^a$ is both divergenceless and uniform along the time direction.

This argument tells us very simply what happens in the case of a single degree of freedom, but, as we have seen before, the divergenceless of $S^a$ is not enough to characterize Hamiltonian mechanics in the general case. In fact we saw that we needed two conditions: determinism and reversibility, which is equivalent to the divergencelessness of $S^a$, and the independence of degrees of freedom. To generalize, then, we should find a generalized mathematical and physical conditions such $\omega_{ab}$ admits $\theta_a$ as potential.

\subsubsection{Potential of the flow - 1 d.o.f. case}

If we compare \ref{rp-cm-lm-contactForm} with \ref{rp-cm-hmd-symplecticForm}, we find that within non-temporal direction the form is identical. Therefore, we can still understand it as quantifying the number of configurations within each d.o.f. and the degree of independence across d.o.f., while adding the idea that the displacement field does not contribute new configurations. Mathematically, the displacement field is the only direction in which the form $\omega_{ab}$ is degenerate. Let us see what those components tell us in terms of the potential. We have:
\begin{equation}\label{rp-cm-lm-potenialConditions}
	\begin{aligned}
		\omega(e^{q^i}, e^{p_j}) &= (-\partial\theta)_{q^i p_j} = -(\partial_{q^i}\theta_{p_j} - \partial_{p_j}\theta_{q^i}) = \delta^i_j \\
		\omega(e^{q^i}, e^{q^j}) &= (-\partial\theta)_{q^i q^j} = -(\partial_{q^i}\theta_{q^j} - \partial_{q^j}\theta_{q^i}) = 0 \\
		\omega(e^{p_i}, e^{p_j}) &= (-\partial\theta)_{p_i p_j} = -(\partial_{p_i}\theta_{p_j} - \partial_{p_j}\theta_{p_i}) = 0
	\end{aligned}
\end{equation}
We can use our gauge freedom to set $\theta_{p_1} = 0$, much in the same way we did before. We now have $\partial_{q^1} \theta_{p_1} = 0$ and, by the first condition, $\partial_{p_1} \theta_{q^1} = 1$. Integrating, we have $\theta_{q^1} = p_1 + g(q^i, p_2, p_3, ..., t)$ where $g$ is an arbitrary function which we can set to zero. Therefore we have:
\begin{equation}
	\theta_a =  \begin{bmatrix}
		p_1 & \theta_{q^2} & \cdots & \theta_{q^n} & 0 & \theta_{p_2} & \cdots & \theta_{p_n} & \theta_{t}
	\end{bmatrix}. 
\end{equation}

Note that the components for the first degree of freedom do not depend on the other degrees of freedom. That is, for all $i>1$, $\partial_{q^i} \theta_{q^1} = \partial_{p_i} \theta_{q^1} = \partial_{q^i} \theta_{p_1} = \partial_{p_i} \theta_{p_1} = 0$. But by using conditions \ref{rp-cm-lm-potenialConditions}, we find that the converse is true as well: the components of all other degrees of freedom do not depend on the first. That is, for all $i>1$, $\partial_{q^1} \theta_{q^i} = \partial_{p_1} \theta_{q^i} = \partial_{q^1} \theta_{p_i} = \partial_{p_1} \theta_{p_i} = 0$.

We can then use, again, our gauge freedom with a function that does not depend on the first two variables to set $\theta_{p_2} = 0$. And, with the same reasoning, we will be able to set $\theta_{q^2} = p_2$. And then, again, find that the first two degrees of freedom do not depend on the others, etc. This will exhaust all d.o.f. and we can set $\theta_t = -H$ as before. This will find \ref{rp-cm-lm-formPotential}.

To recap, we have found that the expression of \ref{rp-cm-lm-contactForm} is equivalent to assuming \ref{assum_detrev} and \ref{assum_indep} and is also equivalent to \ref{rp-cm-lm-formPotential}. Note that the expression of $\omega_{ab}$ and $\theta_a$ are in terms of specific coordinates, while the fact that $\omega_{ab}$ is the exterior derivative of $\theta_a$, instead, is coordinate independent. However, there is a characterization of $\omega_{ab}$ that is fully coordinate independent.

Since $\omega_{ab}$ admits a potential, its exterior derivative is zero. We also have that the displacement field identify the only degenerate direction. That is, if $v^a\omega_{ab} = 0$ then $v^a = f S^a$ for some scalar function $f$. Physically, this means that temporal displacement is the only direction that does not provide independent configurations. It turns out that these are enough conditions to find canonical coordinates $[ q^i, p_i, t]$ such that we can express $\omega_{ab}$ as \ref{rp-cm-lm-contactForm}.\footnote{This result is known as the Darboux's theorem. Unfortunately we haven't been able to find a proof that is short and/or physically significant, though we suspect such a proof should exist.} Therefore
\begin{equation}\label{rp-cm-lm-condSymplecticFormDef}
	\tag{HM-12}
	\parbox{5in}{The two-form $\omega_{ab}$ that quantifies independent configurations is closed \\ (i.e. zero exterior derivative)  and its only direction of degeneracy \\ is identified by the displacement field $S^a$}
\end{equation}
is an equivalent characterization of Hamiltonian mechanics in the extended phase space.

The above condition must imply both \ref{assum_detrev} and \ref{assum_detrev}. We can break down the two contributions in the following way. Assumption \ref{assum_detrev} tells us we have a way to measure the flow of the evolutions over a hyper-surface, which corresponds to a $n-1$-form $\Omega_{a_1 \cdots a_{2n}}$ such that:
\begin{equation}
	S^{a_1} \Omega_{a_1 \cdots a_{2n}} = 0.
\end{equation}
If the space is charted with position, momentum and time, we can write:
\begin{equation}
	\begin{aligned}
	&\Omega_{a_1 \cdots a_{2n}} = \epsilon_{a_1 \cdots a_{2n}a_{2n+1}} S^{a_{2n+1}} \\
	&\int \epsilon_{a_1 \cdots a_{2n}a_{2n+1}} S^{a_{2n+1}} d\xi^{a_1}\cdots d\xi^{a_n} = \int \Omega_{a_1 \cdots a_{2n}} d\xi^{a_1}\cdots d\xi^{a_n}.
	\end{aligned}
\end{equation}
This means that the overall flow is conserved, but the independence of d.o.f. is not. Note that, under \ref{assum_detrev}, we have one and only one evolution for each state, therefore measuring the flow of evolutions is equivalent to measuring the number of states that travel through the surface. If we add \ref{assum_indep}, then we can write:
\begin{equation}
	\Omega_{a_1 \cdots a_{2n}} = \omega_{a_1 a_2} \wedge \cdots \wedge \omega_{a_{2n-1} a_{2n}}.
\end{equation}
This tells us that the total state count becomes the product of the configurations along each degree of freedom.

\subsubsection{Lagrangian and action}

Now that we have seen how the geometry works in the extended phase space, we can go back to the Lagrangian and the action. Suppose that we have a path $\gamma$, not necessarily an actual evolution. It still has the feature that it traverses all time once and only once, and therefore we can write
\begin{equation}
	\begin{aligned}
		\gamma &= [q^i(t), p_i(t), t] \\
		d\gamma &= d_t \xi^a \, dt.
	\end{aligned}
\end{equation}
Note that, in this case, the displacement is along the generic path $\gamma$, which is not necessarily an actual evolution, and therefore it is not necessary that $S^a = d_t \xi^a$. We have:
\begin{equation}
	\begin{aligned}
		L &= p_i v^i - H = \theta_a d_t \xi^a \\
		\mathcal{A}[\gamma] &= \int_\gamma L dt =  \int_\gamma \theta_a d_t \xi^a dt = \int_\gamma \theta_a d\xi^a = \int_\gamma \theta d\gamma
	\end{aligned}
\end{equation}
This tells us that the action is simply the line integral of the vector potential $\theta_a$ of the form $\omega_{ab}$. But the vector potential is not a physical quantity: it depends on a choice of gauge, which does not correspond to a physically well-defined object. The value of the action, and therefore of the Lagrangian, is not itself physically meaningful. So, how is it that we can use something that, in principle, is unphysical to derive the physical laws?

Note that the laws are not expressed in terms of the action, but in terms of the variation of the action. Let $\gamma'$, then, be a small variation of the path $\gamma$ with the same endpoint. Note that $\gamma$ and $\gamma'$ form a closed loop. Take a surface $\Sigma$ that is enclosed by that boundary, we can use Stoke's theorem:
\begin{equation}
	\begin{aligned}
		\delta \mathcal{A}[\gamma] &= \delta \int_\gamma L dt = \delta \int_\gamma \theta_a d\xi^a = \int_{d\Sigma} \partial_b \wedge \theta_a  d\xi^a d\eta^b = \int_{d\Sigma} \partial_b \wedge \theta_a d\xi^a d\eta^b, 
	\end{aligned}
\end{equation}
where $d\eta^b$ is the displacement of the point from $\gamma$ to the variation $\gamma'$. Now, suppose that we want 

Intuitively, deterministic and reversible evolution is responsible for the fact that $S^a$ is a degenerate direction of $\omega_{ab}$. In fact, if the number of states is not conserved in time, the displacement 


 Having introduced the wedge product, we can write the second conditions as
That is, $\Omega_{a_1 \cdots a_{2n}}$ is the form that counts the number of states on an hyper-volume of the extended phase-space while $\omega_{ab}$ is the form that counts the number of configurations for each independent degree of freedom. The mathematical statement tells us that the volume is equal to the product of the sides, which, as we saw, is equivalent to stating that the whole space is characterized by independent degrees of freedom.

To generalize, then, we need to understand two things then. First is the relationship between the displacement field $S^a$ and $\omega_{ab}$ and then the relationship between the properties $\omega_{ab}$ must have such that it has it admits $\theta_a$ as covector potential.

As we said before, the displacement should not give us new independent configurations, therefore
\begin{equation}\label{rp-cp-lm-displacementPerpForm}
	\begin{aligned}
	&S^{a_1} \Omega_{a_1 \cdots a_{2n}} = 0 \\
	&S^a \omega_{ab} = 0.
	\end{aligned}
\end{equation}
Note that, if \ref{rp-cm-lm-dofIndependenceForm} is satisfied, the previous two conditions are equivalent. However, this is not the only condition we want. In fact, we want the flow of $S^a$ to be in terms of the count of states. Therefore we want
\begin{equation}
	\epsilon_{a_1 \cdots a_{2n}a_{2n+1}} S^{a_{2n+1}} = \Omega_{a_1 \cdots a_{2n}},
\end{equation}
where $\epsilon$ is again the fully anti-symmetric Levi-Civita symbol. Note that, in the one d.o.f. case, the above reduces to \ref{rp-cm-lm-displacementFormComponents}. This condition automatically imply \ref{rp-cp-lm-displacementPerpForm} because
\begin{equation}
	\begin{aligned}
		S^{a_1} \Omega_{a_1 \cdots a_{2n}} &= S^{a_1}\epsilon_{a_1 \cdots a_{2n}a_{2n+1}} S^{a_{2n+1}} = S^{a_{2n+1}}\epsilon_{a_1 \cdots a_{2n}a_{2n+1}} S^{a_1} \\
		&= - S^{a_{2n+1}}\epsilon_{a_{2n+1} \cdots a_{2n}a_1} S^{a_1} = 0.
	\end{aligned}
\end{equation}
Since $\epsilon$ is fully anti-symmetric, contracting it with the same vector twice gives us zero. Geometrically, $\epsilon$ gives us the volume element measures in coordinates, but if two sides of a parallelepiped coincide, the volume is zero.

To calculate the flow of $S^a$, then, we can use
\begin{equation}
	\int \epsilon_{a_1 \cdots a_{2n}a_{2n+1}} S^{a_{2n+1}} d\xi^{a_1}\cdots d\xi^{a_n} = \int \Omega_{a_1 \cdots a_{2n}} d\xi^{a_1}\cdots d\xi^{a_n}
\end{equation}
This means that defining the flow of $S^a$ implies defining the form $\Omega_{a_1 \cdots a_{2n}}$. In terms of covariance, note that $\Omega_{a_1 \cdots a_{2n}}$ is a $2n$ form, a fully anti-symmetric tensor. However, $\epsilon_{a_1 \cdots a_{2n+1}}$ is not a form. It returns the volume as measured in each specific coordinates. For example, in two dimensional polar coordinates, it will return the volume in units of distance times angle. However, note that $S^a$ is special in terms of units as well. In fact, recall that $S^a=d_t \xi^a$. Time here has two roles: as the coordinate $\xi^t$ and as the affine parameter for the curve that represent an evolution. Therefore, technically, $S^a$ on the extended phase space is not a vector either. While $\epsilon_{a_1 \cdots a_{2n+1}}$ and $S^a$ are not tensors by themselves, their contraction is.



Subsequently, requiring the flow of $S^a$ is zero over each hypersurface, that the evolution is deterministic and reversible, tells us $\Omega_{a_1 \cdots a_{2n}}$ is an exact form and admits a vector potential. Additionally, requiring $\Omega_{a_1 \cdots a_{2n}} = \bigwedge^n \omega_{ab}$, that the system is described by independent d.o.f., gives us the $\omega_{ab}$ which is going to be 


The key here is $S^a$ is something we want to integrate over hyper-surfaces to calculate a flow. Then, more properly, we should not understand $S^a$ as a $n-1$ form. In the single degree of freedom , $\omega_{ab}$ is exactly that form: we have a three dimensional space so an $(n-1)$-form is a two-form. The idea is that $\omega_{qp}$ represents the flow through a surface at constant position and momentum, which is the flow along time, which is the flow along time $S^t$. The same goes for the other components.

In the general case, then, we would need a form $\Omega_{a_1 a_2 \cdots a_{n-1}}$ to represent the flow of states in the extended phase space. The exterior derivative of this flow is zero, which generalizes the idea that the divergence  $\omega{ab}$ 

% We want to have a differential operator that does that in general. Since $\nabla = \partial_i$, we are looking for something like $\partial_{i_0} \theta_{i_1 i_2 \cdots i_k}$. This does not quite work as this takes the derivative in only one direction. When we integrate, we would find the difference between the boundaries on one dimension alone. If we imagine integrating along a parallelepiped, we need to a derivative along each dimension and, for each dimensions, evaluate the difference of the form along facing sides. The sum of all the sides becomes the surface integral on the boundary. Let's use $\nabla_i$ for this operations, we have:
%\begin{equation}
%	\begin{aligned}
%		\int_\lambda \nabla_i T dx^i &= \int_\lambda \partial_{i} T dx^i = \int_{A}^{B} \partial_u T du = \int_{A}^{B} dT = T(B) - T(A) \\
%		\int_\Sigma \nabla_i F_{j} dx^i dx^j &= \int_\Sigma ( \partial_{i} F_{j} - \partial_{j} F_{i}) dx^i dx^j =\int_{u_1}^{u^2} \int_{v_1}^{v^2} ( \partial_u F_v - \partial_v F_u) du dv \\
%		&= \left.\int_{v_1}^{v^2} F_{j} dv^j \right|_{u_1}^{u^2} - \left.\int_{u_1}^{u^2} F_{i} du^i \right|_{v_1}^{v^2} = \int_{\partial \Sigma} F_i dx^i \\
%		\int_V \nabla_i B_{jk} dx^i dx^j dx^k &= \int_V (\partial_{i} B_{jk} + \partial_{j} B_{ki} + \partial_{k} B_{ij}) dx^i dx^j dx^k \\
%		&= \int_{u_1}^{u^2} \int_{v_1}^{v^2} \int_{w_1}^{w^2} (\partial_{u} B_{vw} + \partial_{v} B_{wu} + \partial_{w} B_{uv}) du dv dw\\
%		&= \int_{\partial V} B_{ij} dx^i dy^j
%	\end{aligned}
%\end{equation}
%What happens is that the operator $\nabla_i$, which is called exterior derivative in differential geometry, applies a derivative in each direction by rotating the indexes. Because of the anti-symmetric nature of forms, we have to take care of the sign. A cycle in $k$ indexes corresponds to $k-1$ swaps. Since each swap corresponding to a minus sign because of the anti-symmetry, if $k$ is odd we end up with even minus signs, which means no sign change, while 

